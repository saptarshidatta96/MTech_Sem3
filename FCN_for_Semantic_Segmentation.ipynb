{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCN for Semantic Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWnqFn0rpDxia2sWBo227M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saptarshidatta96/MTech_Sem3/blob/main/FCN_for_Semantic_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==1.14"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "_WDT67Zk9BOp",
        "outputId": "8bb7ee51-6f30-4647-9445-a9b55a3bb155"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==1.14\n",
            "  Downloading tensorflow_gpu-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (377.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 377.1 MB 8.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.44.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.0.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (0.37.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (1.21.6)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14) (0.5.3)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.8.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow-gpu==1.14) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires tensorboard<2.9,>=2.8, but you have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5MJzQeYpgmU",
        "outputId": "04c5e19c-23c6-4fa1-f804-89389b3dfe3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/gdrive/MyDrive/cityscapes_data.zip\" -d \"/content/\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HGSDuaXppiv",
        "outputId": "6376d249-e467-46fd-aeee-067061ae9eba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/cityscapes_data.zip\n",
            "replace /content/cityscapes_data/train/1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Conv2DTranspose, Cropping2D\n",
        "from keras.layers import Input, Add, Dropout, Permute, add"
      ],
      "metadata": {
        "id": "DgX9c_2uucpV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "id": "KVXq7TSKJOYZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists('/content/train') == False:\n",
        "  os.mkdir('/content/train')\n",
        "if os.path.exists('/content/val/') == False:\n",
        "  os.mkdir('/content/val/')\n",
        "\n"
      ],
      "metadata": {
        "id": "ZBl9dQo1ucEd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(path):\n",
        "  \n",
        "  for folder in os.listdir(path):\n",
        "      img_path = os.path.join(path, folder)\n",
        "      for img in os.listdir(img_path):\n",
        "  \n",
        "        img_src = os.path.join(img_path, img)\n",
        "        imgg = cv2.imread(img_src)\n",
        "        height, width, channels = imgg.shape\n",
        "        x = imgg[0:height, 0:width//2]\n",
        "        y = imgg[0:height, width//2:width]\n",
        "        #cv2.imwrite('/content/ctscp_data/{}/label/{}'.format(folder, img), y)\n",
        "        cv2.imwrite('/content/{}/{}'.format(folder, img),x)\n"
      ],
      "metadata": {
        "id": "SvhRAr977izQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_data('/content/cityscapes_data')"
      ],
      "metadata": {
        "id": "yh_AJgPg9nKi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shutil.rmtree('/content/cityscapes_data')"
      ],
      "metadata": {
        "id": "kfgaCdQo_p-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "You should not edit helper.py as part of your submission.\n",
        "\n",
        "This file is used primarily to download vgg if it has not yet been,\n",
        "give you the progress of the download, get batches for your training,\n",
        "as well as around generating and saving the image outputs.\n",
        "'''\n",
        "\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import os.path\n",
        "import scipy.misc\n",
        "import shutil\n",
        "import zipfile\n",
        "import time\n",
        "from glob import glob\n",
        "from urllib.request import urlretrieve\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class DLProgress(tqdm):\n",
        "\t\"\"\"\n",
        "\tReport download progress to the terminal.\n",
        "\t:param tqdm: Information fed to the tqdm library to estimate progress.\n",
        "\t\"\"\"\n",
        "\tlast_block = 0\n",
        "\n",
        "\tdef hook(self, block_num=1, block_size=1, total_size=None):\n",
        "\t\t\"\"\"\n",
        "\t\tStore necessary information for tracking progress.\n",
        "\t\t:param block_num: current block of the download\n",
        "\t\t:param block_size: size of current block\n",
        "\t\t:param total_size: total download size, if known\n",
        "\t\t\"\"\"\n",
        "\t\tself.total = total_size\n",
        "\t\tself.update((block_num - self.last_block) * block_size)  # Updates progress\n",
        "\t\tself.last_block = block_num\n",
        "\n",
        "\n",
        "def maybe_download_pretrained_vgg(data_dir):\n",
        "\t\"\"\"\n",
        "\tDownload and extract pretrained vgg model if it doesn't exist\n",
        "\t:param data_dir: Directory to download the model to\n",
        "\t\"\"\"\n",
        "\tvgg_filename = 'vgg.zip'\n",
        "\tvgg_path = os.path.join(data_dir, 'vgg')\n",
        "\tvgg_files = [\n",
        "\t\tos.path.join(vgg_path, 'variables/variables.data-00000-of-00001'),\n",
        "\t\tos.path.join(vgg_path, 'variables/variables.index'),\n",
        "\t\tos.path.join(vgg_path, 'saved_model.pb')]\n",
        "\n",
        "\tmissing_vgg_files = [vgg_file for vgg_file in vgg_files if not os.path.exists(vgg_file)]\n",
        "\tif missing_vgg_files:\n",
        "\t\t# Clean vgg dir\n",
        "\t\tif os.path.exists(vgg_path):\n",
        "\t\t\tshutil.rmtree(vgg_path)\n",
        "\t\tos.makedirs(vgg_path)\n",
        "\n",
        "\t\t# Download vgg\n",
        "\t\tprint('Downloading pre-trained vgg model...')\n",
        "\t\twith DLProgress(unit='B', unit_scale=True, miniters=1) as pbar:\n",
        "\t\t\turlretrieve(\n",
        "\t\t\t\t'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip',\n",
        "\t\t\t\tos.path.join(vgg_path, vgg_filename),\n",
        "\t\t\t\tpbar.hook)\n",
        "\n",
        "\t\t# Extract vgg\n",
        "\t\tprint('Extracting model...')\n",
        "\t\tzip_ref = zipfile.ZipFile(os.path.join(vgg_path, vgg_filename), 'r')\n",
        "\t\tzip_ref.extractall(data_dir)\n",
        "\t\tzip_ref.close()\n",
        "\n",
        "\t\t# Remove zip file to save space\n",
        "\t\tos.remove(os.path.join(vgg_path, vgg_filename))\n",
        "\n",
        "\n",
        "def gen_batch_function(data_folder, image_shape):\n",
        "\t\"\"\"\n",
        "\tGenerate function to create batches of training data\n",
        "\t:param data_folder: Path to folder that contains all the datasets\n",
        "\t:param image_shape: Tuple - Shape of image\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\tdef get_batches_fn(batch_size):\n",
        "\t\t\"\"\"\n",
        "\t\tCreate batches of training data\n",
        "\t\t:param batch_size: Batch Size\n",
        "\t\t:return: Batches of training data\n",
        "\t\t\"\"\"\n",
        "\t\t# Grab image and label paths\n",
        "\t\timage_paths = glob(os.path.join(data_folder, 'image_2', '*.png'))\n",
        "\t\tlabel_paths = {\n",
        "\t\t\tre.sub(r'_(lane|road)_', '_', os.path.basename(path)): path\n",
        "\t\t\tfor path in glob(os.path.join(data_folder, 'gt_image_2', '*_road_*.png'))}\n",
        "\t\tbackground_color = np.array([255, 0, 0])\n",
        "\n",
        "\t\t# Shuffle training data\n",
        "\t\trandom.shuffle(image_paths)\n",
        "\t\t# Loop through batches and grab images, yielding each batch\n",
        "\t\tfor batch_i in range(0, len(image_paths), batch_size):\n",
        "\t\t\timages = []\n",
        "\t\t\tgt_images = []\n",
        "\t\t\tfor image_file in image_paths[batch_i:batch_i+batch_size]:\n",
        "\t\t\t\tgt_image_file = label_paths[os.path.basename(image_file)]\n",
        "\t\t\t\t# Re-size to image_shape\n",
        "\t\t\t\timage = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
        "\t\t\t\tgt_image = scipy.misc.imresize(scipy.misc.imread(gt_image_file), image_shape)\n",
        "\n",
        "\t\t\t\t# Create \"one-hot-like\" labels by class\n",
        "\t\t\t\tgt_bg = np.all(gt_image == background_color, axis=2)\n",
        "\t\t\t\tgt_bg = gt_bg.reshape(*gt_bg.shape, 1)\n",
        "\t\t\t\tgt_image = np.concatenate((gt_bg, np.invert(gt_bg)), axis=2)\n",
        "\n",
        "\t\t\t\timages.append(image)\n",
        "\t\t\t\tgt_images.append(gt_image)\n",
        "\n",
        "\t\t\tyield np.array(images), np.array(gt_images)\n",
        "\treturn get_batches_fn\n",
        "\n",
        "\n",
        "def gen_test_output(sess, logits, keep_prob, image_pl, data_folder, image_shape):\n",
        "\t\"\"\"\n",
        "\tGenerate test output using the test images\n",
        "\t:param sess: TF session\n",
        "\t:param logits: TF Tensor for the logits\n",
        "\t:param keep_prob: TF Placeholder for the dropout keep probability\n",
        "\t:param image_pl: TF Placeholder for the image placeholder\n",
        "\t:param data_folder: Path to the folder that contains the datasets\n",
        "\t:param image_shape: Tuple - Shape of image\n",
        "\t:return: Output for for each test image\n",
        "\t\"\"\"\n",
        "\tfor image_file in glob(os.path.join(data_folder, 'image_2', '*.png')):\n",
        "\t\timage = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
        "\n",
        "\t\t# Run inference\n",
        "\t\tim_softmax = sess.run(\n",
        "\t\t\t[tf.nn.softmax(logits)],\n",
        "\t\t\t{keep_prob: 1.0, image_pl: [image]})\n",
        "\t\t# Splice out second column (road), reshape output back to image_shape\n",
        "\t\tim_softmax = im_softmax[0][:, 1].reshape(image_shape[0], image_shape[1])\n",
        "\t\t# If road softmax > 0.5, prediction is road\n",
        "\t\tsegmentation = (im_softmax > 0.5).reshape(image_shape[0], image_shape[1], 1)\n",
        "\t\t# Create mask based on segmentation to apply to original image\n",
        "\t\tmask = np.dot(segmentation, np.array([[0, 255, 0, 127]]))\n",
        "\t\tmask = scipy.misc.toimage(mask, mode=\"RGBA\")\n",
        "\t\tstreet_im = scipy.misc.toimage(image)\n",
        "\t\tstreet_im.paste(mask, box=None, mask=mask)\n",
        "\n",
        "\t\tyield os.path.basename(image_file), np.array(street_im)\n",
        "\n",
        "\n",
        "def save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image):\n",
        "\t\"\"\"\n",
        "\tSave test images with semantic masks of lane predictions to runs_dir.\n",
        "\t:param runs_dir: Directory to save output images\n",
        "\t:param data_dir: Path to the directory that contains the datasets\n",
        "\t:param sess: TF session\n",
        "\t:param image_shape: Tuple - Shape of image\n",
        "\t:param logits: TF Tensor for the logits\n",
        "\t:param keep_prob: TF Placeholder for the dropout keep probability\n",
        "\t:param input_image: TF Placeholder for the image placeholder\n",
        "\t\"\"\"\n",
        "\t# Make folder for current run\n",
        "\toutput_dir = os.path.join(runs_dir, str(time.time()))\n",
        "\tif os.path.exists(output_dir):\n",
        "\t\tshutil.rmtree(output_dir)\n",
        "\tos.makedirs(output_dir)\n",
        "\n",
        "\t# Run NN on test images and save them to HD\n",
        "\tprint('Training Finished. Saving test images to: {}'.format(output_dir))\n",
        "\timage_outputs = gen_test_output(\n",
        "\t\tsess, logits, keep_prob, input_image, os.path.join(data_dir, 'data_road/testing'), image_shape)\n",
        "\tfor name, image in image_outputs:\n",
        "\t\tscipy.misc.imsave(os.path.join(output_dir, name), image)"
      ],
      "metadata": {
        "id": "h6ZEqN0oNYT9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 60\n",
        "BATCH_SIZE = 25\n",
        "NUM_CLASSES=29\n",
        "IS_TRAIN = True\n",
        "KEEP_PROB = 0.5\n",
        "LEARNING_RATE = 0.0005\n",
        "L2_REG = 1e-2\n",
        "STDEV = 1e-3\n",
        "\n",
        "DATA_DIR = '/content/train'\n",
        "VIDEO_DIR = 'data\\leftImg8bit_demoVideo\\leftImg8bit\\demoVideo\\stuttgart_02'\n",
        "RUNS_DIR = 'runs'\n",
        "\n",
        "DEFAULT_SAVE_DIR = ''\n",
        "PRETRAINED_MODEL_DIR = ''"
      ],
      "metadata": {
        "id": "mcY9PapRNfy1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from collections import namedtuple\n",
        "import random\n",
        "import numpy as np\n",
        "import os.path\n",
        "import scipy.misc\n",
        "import os\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "import time\n",
        "\n",
        "def get_label_info():\n",
        "    \"\"\"\n",
        "    Retrieve the class names and label values for the selected dataset.\n",
        "    Must be in CSV format!\n",
        "    # Arguments\n",
        "        csv_path: The file path of the class dictionairy\n",
        "\n",
        "    # Returns\n",
        "        Two lists: one for the class names and the other for the label values\n",
        "    \"\"\"\n",
        "\n",
        "    # a label and all meta information\n",
        "    Label = namedtuple('Label', [\n",
        "\n",
        "        'name',  # The identifier of this label, e.g. 'car', 'person', ... .\n",
        "        # We use them to uniquely name a class\n",
        "\n",
        "        'id',  # An integer ID that is associated with this label.\n",
        "        # The IDs are used to represent the label in ground truth images\n",
        "        # An ID of -1 means that this label does not have an ID and thus\n",
        "        # is ignored when creating ground truth images (e.g. license plate).\n",
        "        # Do not modify these IDs, since exactly these IDs are expected by the\n",
        "        # evaluation server.\n",
        "\n",
        "        'trainId',  # Feel free to modify these IDs as suitable for your method. Then create\n",
        "        # ground truth images with train IDs, using the tools provided in the\n",
        "        # 'preparation' folder. However, make sure to validate or submit results\n",
        "        # to our evaluation server using the regular IDs above!\n",
        "        # For trainIds, multiple labels might have the same ID. Then, these labels\n",
        "        # are mapped to the same class in the ground truth images. For the inverse\n",
        "        # mapping, we use the label that is defined first in the list below.\n",
        "        # For example, mapping all void-type classes to the same ID in training,\n",
        "        # might make sense for some approaches.\n",
        "        # Max value is 255!\n",
        "\n",
        "        'category',  # The name of the category that this label belongs to\n",
        "\n",
        "        'categoryId',  # The ID of this category. Used to create ground truth images\n",
        "        # on category level.\n",
        "\n",
        "        'hasInstances',  # Whether this label distinguishes between single instances or not\n",
        "\n",
        "        'ignoreInEval',  # Whether pixels having this class as ground truth label are ignored\n",
        "        # during evaluations or not\n",
        "\n",
        "        'color',  # The color of this label\n",
        "    ])\n",
        "\n",
        "    labels = [\n",
        "        #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
        "        Label('unlabeled', 0, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        Label('ego vehicle', 1, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        Label('rectification border', 2, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        Label('out of roi', 3, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        Label('static', 4, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        Label('dynamic', 5, 255, 'void', 0, False, True, (111, 74, 0)),\n",
        "        Label('ground', 6, 255, 'void', 0, False, True, (81, 0, 81)),\n",
        "        Label('road', 7, 0, 'flat', 1, False, False, (128, 64, 128)),\n",
        "        Label('sidewalk', 8, 1, 'flat', 1, False, False, (244, 35, 232)),\n",
        "        Label('parking', 9, 255, 'flat', 1, False, True, (250, 170, 160)),\n",
        "        Label('rail track', 10, 255, 'flat', 1, False, True, (230, 150, 140)),\n",
        "        Label('building', 11, 2, 'construction', 2, False, False, (70, 70, 70)),\n",
        "        Label('wall', 12, 3, 'construction', 2, False, False, (102, 102, 156)),\n",
        "        Label('fence', 13, 4, 'construction', 2, False, False, (190, 153, 153)),\n",
        "        Label('guard rail', 14, 255, 'construction', 2, False, True, (180, 165, 180)),\n",
        "        Label('bridge', 15, 255, 'construction', 2, False, True, (150, 100, 100)),\n",
        "        Label('tunnel', 16, 255, 'construction', 2, False, True, (150, 120, 90)),\n",
        "        Label('pole', 17, 5, 'object', 3, False, False, (153, 153, 153)),\n",
        "        Label('polegroup', 18, 255, 'object', 3, False, True, (153, 153, 153)),\n",
        "        Label('traffic light', 19, 6, 'object', 3, False, False, (250, 170, 30)),\n",
        "        Label('traffic sign', 20, 7, 'object', 3, False, False, (220, 220, 0)),\n",
        "        Label('vegetation', 21, 8, 'nature', 4, False, False, (107, 142, 35)),\n",
        "        Label('terrain', 22, 9, 'nature', 4, False, False, (152, 251, 152)),\n",
        "        Label('sky', 23, 10, 'sky', 5, False, False, (70, 130, 180)),\n",
        "        Label('person', 24, 11, 'human', 6, True, False, (220, 20, 60)),\n",
        "        Label('rider', 25, 12, 'human', 6, True, False, (255, 0, 0)),\n",
        "        Label('car', 26, 13, 'vehicle', 7, True, False, (0, 0, 142)),\n",
        "        Label('truck', 27, 14, 'vehicle', 7, True, False, (0, 0, 70)),\n",
        "        Label('bus', 28, 15, 'vehicle', 7, True, False, (0, 60, 100)),\n",
        "        Label('caravan', 29, 255, 'vehicle', 7, True, True, (0, 0, 90)),\n",
        "        Label('trailer', 30, 255, 'vehicle', 7, True, True, (0, 0, 110)),\n",
        "        Label('train', 31, 16, 'vehicle', 7, True, False, (0, 80, 100)),\n",
        "        Label('motorcycle', 32, 17, 'vehicle', 7, True, False, (0, 0, 230)),\n",
        "        Label('bicycle', 33, 18, 'vehicle', 7, True, False, (119, 11, 32)),\n",
        "        Label('license plate', -1, -1, 'vehicle', 7, False, True, (0, 0, 142)),\n",
        "    ]\n",
        "\n",
        "    seen = set()\n",
        "    label_list = list(map(lambda x : x[7], labels))\n",
        "    label_values = [x for x in label_list if not (x in seen or seen.add(x))]\n",
        "\n",
        "    return label_values\n",
        "\n",
        "def get_data(data_path):\n",
        "    train_path = data_path + '/leftImg8bit/train/'\n",
        "    trainy_path = data_path + '/sky-data/train/'\n",
        "\n",
        "    val_path = data_path + '/leftImg8bit/val/'\n",
        "    valy_path = data_path + '/sky-data/val/'\n",
        "\n",
        "    train_batch = glob(os.path.join(train_path, '*/*.png'))\n",
        "\n",
        "    trainy_batch = glob(os.path.join(trainy_path, '*/*color.png'))\n",
        "    val_batch = glob(os.path.join(val_path, '*/*.png'))\n",
        "\n",
        "    valy_batch = glob(os.path.join(valy_path, '*/*color.png'))\n",
        "\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    X_val = []\n",
        "    y_val = []\n",
        "\n",
        "    print('Loading X_Train..')\n",
        "    for sample in train_batch:\n",
        "        img_path = sample\n",
        "        x = load_image(img_path)\n",
        "        x = cv2.resize(x, dsize=(512, 256))\n",
        "        X_train.append(x)\n",
        "\n",
        "    print('Loading Y_Train..')\n",
        "    for sample in trainy_batch:\n",
        "        img_path = sample\n",
        "        x = load_image(img_path)\n",
        "        x = cv2.resize(x, dsize=(512, 256))\n",
        "        y_train.append(x)\n",
        "\n",
        "    print('Loading X_Validation..')\n",
        "    for sample in val_batch:\n",
        "        img_path = sample\n",
        "        x = load_image(img_path)\n",
        "        x = cv2.resize(x, dsize=(512, 256))\n",
        "        X_val.append(x)\n",
        "\n",
        "    print('Loading Y_Validation..')\n",
        "    for sample in valy_batch:\n",
        "        img_path = sample\n",
        "        x = load_image(img_path)\n",
        "        x = cv2.resize(x, dsize=(512, 256))\n",
        "        y_val.append(x)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val\n",
        "\n",
        "def one_hot_it(label, label_values):\n",
        "    semantic_encoding = []\n",
        "    # c = np.logical_and(np.not_equal(label, label_values[0]), np.not_equal(label, label_values[1]))\n",
        "    # mask = np.any(c, axis=-1)\n",
        "    # semantic_map.append(mask)\n",
        "    for color in label_values:\n",
        "        equality = np.equal(label, color)\n",
        "        class_map = np.all(equality, axis=-1)\n",
        "        semantic_encoding.append(class_map)\n",
        "    semantic_map = np.stack(semantic_encoding, axis=-1)\n",
        "\n",
        "    return semantic_map\n",
        "\n",
        "\n",
        "def reverse_one_hot(image):\n",
        "    x = np.argmax(image, axis=-1)\n",
        "    return x\n",
        "\n",
        "\n",
        "def colour_code_segmentation(image, label_values):\n",
        "\n",
        "    label_matrix = np.array(label_values)\n",
        "    x = label_matrix[image.astype(int)]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def load_image(path):\n",
        "    image = cv2.cvtColor(cv2.imread(path, -1), cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_annotation(path):\n",
        "    image = cv2.imread(path, -1)\n",
        "    return image\n",
        "\n",
        "\n",
        "def flip_image(image, measurement, flip_probability=1.0):\n",
        "    if random.random() <= flip_probability:\n",
        "        image = cv2.flip(image, 1)\n",
        "        measurement *= -1\n",
        "    return image, measurement\n",
        "\n",
        "\n",
        "# Randomly crop the image to a specific size. For data augmentation\n",
        "def random_crop(image, label, crop_height, crop_width):\n",
        "    if (image.shape[0] != label.shape[0]) or (image.shape[1] != label.shape[1]):\n",
        "        raise Exception('Image and label must have the same dimensions!')\n",
        "\n",
        "    if (crop_width <= image.shape[1]) and (crop_height <= image.shape[0]):\n",
        "        x = random.randint(0, image.shape[1] - crop_width)\n",
        "        y = random.randint(0, image.shape[0] - crop_height)\n",
        "\n",
        "        if len(label.shape) == 3:\n",
        "            return image[y:y + crop_height, x:x + crop_width, :], label[y:y + crop_height, x:x + crop_width, :]\n",
        "        else:\n",
        "            return image[y:y + crop_height, x:x + crop_width, :], label[y:y + crop_height, x:x + crop_width]\n",
        "    else:\n",
        "        raise Exception('Crop shape (%d, %d) exceeds image dimensions (%d, %d)!' % (\n",
        "            crop_height, crop_width, image.shape[0], image.shape[1]))\n",
        "\n",
        "\n",
        "def random_shadow(image):\n",
        "    \"\"\"\n",
        "    Generates and adds random shadow\n",
        "    \"\"\"\n",
        "    rand_width_scal_1 = np.random.rand()\n",
        "    IMAGE_HEIGHT, IMAGE_WIDTH, _ = image.shape\n",
        "    x1, y1 = IMAGE_WIDTH * rand_width_scal_1, 0\n",
        "    rand_width_scal_2 = np.random.rand()\n",
        "    x2, y2 = IMAGE_WIDTH * rand_width_scal_2, IMAGE_HEIGHT\n",
        "    xn, yn = np.mgrid[0:IMAGE_HEIGHT, 0:IMAGE_WIDTH]\n",
        "    mask = np.zeros_like(image[:, :, 1])\n",
        "    mask[(yn - y1) * (x2 - x1) - (y2 - y1) * (xn - x1) > 0] = 1\n",
        "\n",
        "    cond = mask == np.random.randint(2)\n",
        "    s_ratio = np.random.uniform(low=0.2, high=0.5)\n",
        "\n",
        "    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
        "    hls[:, :, 1][cond] = hls[:, :, 1][cond] * s_ratio\n",
        "    return cv2.cvtColor(hls, cv2.COLOR_HLS2RGB)\n",
        "\n",
        "def random_brightness(image):\n",
        "    \"\"\"\n",
        "    Randomly adjust brightness of the image.\n",
        "    HSV (Hue, Saturation, Value) is also called HSB ('B' for Brightness).\n",
        "    \"\"\"\n",
        "\n",
        "    hsv_channel = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "    brightness_scalar = np.random.rand()\n",
        "    ratio = 1.0 + 0.4 * (brightness_scalar - 0.5)\n",
        "    hsv_channel[:, :, 2] = hsv_channel[:, :, 2] * ratio\n",
        "    return cv2.cvtColor(hsv_channel, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "\n",
        "def rotation(input_image, output_image, degrees):\n",
        "    angle = random.uniform(-1*degrees, degrees)\n",
        "    M = cv2.getRotationMatrix2D((input_image.shape[1]//2, input_image.shape[0]//2), angle, 1.0)\n",
        "    input_image = cv2.warpAffine(input_image, M, (input_image.shape[1], input_image.shape[0]), flags=cv2.INTER_NEAREST)\n",
        "    output_image = cv2.warpAffine(output_image, M, (output_image.shape[1], output_image.shape[0]), flags=cv2.INTER_NEAREST)\n",
        "    return input_image, output_image\n",
        "\n",
        "def brightness(image, bright_0_1):\n",
        "            factor = 1.0 + random.uniform(-1.0*bright_0_1, bright_0_1)\n",
        "            table = np.array([((i / 255.0) * factor) * 255 for i in np.arange(0, 256)]).astype(np.uint8)\n",
        "            image = cv2.LUT(image, table)\n",
        "            return image\n",
        "\n",
        "\n",
        "def data_augmentation(input_image, output_image):\n",
        "\n",
        "    if random.randint(0, 1):\n",
        "        input_image = brightness(input_image, 0.5)\n",
        "\n",
        "    if random.randint(0, 1):\n",
        "        input_image = random_shadow(input_image)\n",
        "\n",
        "\n",
        "    return input_image, output_image\n",
        "\n",
        "\n",
        "def gen_batch_function(samplesX, samplesY, label_values, batch_size=1, is_train=True):\n",
        "    \"\"\"\n",
        "    F function to create batches of training data\n",
        "    :param data_folder: Path to folder that contains all the datasets\n",
        "    :param image_shape: Tuple - Shape of image\n",
        "    :return:\"\"\"\n",
        "\n",
        "    samplesX, samplesY = shuffle(samplesX, samplesY)\n",
        "\n",
        "    # Shuffle training data\n",
        "\n",
        "    num_samples = len(samplesX)\n",
        "\n",
        "    if (batch_size == -1):\n",
        "        batch_size = num_samples\n",
        "\n",
        "    # Loop through batches and grab images, yielding each batch\n",
        "    for batch_i in range(0, num_samples, batch_size):\n",
        "\n",
        "        X_train = samplesX[batch_i:batch_i + batch_size]\n",
        "        y_train = samplesY[batch_i:batch_i + batch_size]\n",
        "\n",
        "        # preprocessing if required\n",
        "        X_f = []\n",
        "        y_f = []\n",
        "        for x, y in zip(X_train, y_train):\n",
        "            if is_train:\n",
        "                x, y = data_augmentation(x, y)\n",
        "\n",
        "            y = np.float32(one_hot_it(y, label_values=label_values))\n",
        "            X_f.append(x)\n",
        "            y_f.append(y)\n",
        "\n",
        "        X_f = np.float32(X_f)\n",
        "        y_f = np.float32(y_f)\n",
        "        yield X_f, y_f\n",
        "\n",
        "def pipeline_final(img, sess, logits, keep_prob, input_image, image_shape, label_values, num_classes=29):\n",
        "    channel = 1\n",
        "    img= cv2.resize(img, dsize=image_shape)\n",
        "    img = np.array([img])\n",
        "    softmax_ = loss = sess.run([logits],\n",
        "                       feed_dict={input_image: img, keep_prob:1})\n",
        "    logits_ = (softmax_[0].reshape(1,image_shape[1],image_shape[0],num_classes))\n",
        "    output_image = reverse_one_hot(logits_[0])\n",
        "\n",
        "    out_vis_image = colour_code_segmentation(output_image, label_values)\n",
        "\n",
        "    a = cv2.cvtColor(np.uint8(out_vis_image), channel)\n",
        "\n",
        "    b = cv2.cvtColor(np.uint8(img[0]), channel)\n",
        "\n",
        "    added_image = cv2.addWeighted(a, 1, b, 1, channel)\n",
        "    added_image = cv2.resize(added_image, image_shape)\n",
        "\n",
        "    return added_image\n",
        "\n",
        "\n",
        "def process(media_dir, sess, logits, keep_prob, input_image, image_shape, label_values):\n",
        "    img = load_image(media_dir)\n",
        "    img = pipeline_final(img, sess, logits, keep_prob, input_image, image_shape, label_values)\n",
        "    return img\n",
        "\n",
        "\n",
        "def gen_test_output(sess, logits, keep_prob, input_image, data_folder, image_shape, label_values):\n",
        "\n",
        "    for image_file in glob(os.path.join(data_folder, '*.png')):\n",
        "        image = process(image_file, sess, logits, keep_prob, input_image, image_shape, label_values)\n",
        "        yield os.path.basename(image_file), image\n",
        "\n",
        "\n",
        "def save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image, label_values):\n",
        "\t\"\"\"\n",
        "\tSave test images with semantic masks of lane predictions to runs_dir.\n",
        "\t:param runs_dir: Directory to save output images\n",
        "\t:param data_dir: Path to the directory that contains the datasets\n",
        "\t:param sess: TF session\n",
        "\t:param image_shape: Tuple - Shape of image\n",
        "\t:param logits: TF Tensor for the logits\n",
        "\t:param keep_prob: TF Placeholder for the dropout keep probability\n",
        "\t:param input_image: TF Placeholder for the image placeholder\n",
        "\t\"\"\"\n",
        "\t# Make folder for current run\n",
        "\toutput_dir = os.path.join(runs_dir, str(time.time()))\n",
        "\tif os.path.exists(output_dir):\n",
        "\t\tshutil.rmtree(output_dir)\n",
        "\tos.makedirs(output_dir)\n",
        "\n",
        "\t# Run NN on test images and save them to HD\n",
        "\tprint('Saving test images to: {}'.format(output_dir))\n",
        "\timage_outputs = gen_test_output(\n",
        "\n",
        "\t\tsess, logits, keep_prob, input_image, data_dir, image_shape, label_values)\n",
        "\tfor name, image in image_outputs:\n",
        "\t\tscipy.misc.imsave(os.path.join(output_dir, name), image)"
      ],
      "metadata": {
        "id": "Aslvbi7YN7KF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os.path\n",
        "import helper\n",
        "import warnings\n",
        "from distutils.version import LooseVersion\n",
        "\n",
        "\n",
        "\n",
        "# Check TensorFlow Version\n",
        "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "\n",
        "# Check for a GPU\n",
        "if not tf.test.gpu_device_name():\n",
        "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
        "else:\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "\n",
        "\n",
        "def load_vgg(sess, vgg_path):\n",
        "    \"\"\"\n",
        "    Load Pretrained VGG Model into TensorFlow.\n",
        "    :param sess: TensorFlow Session\n",
        "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
        "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "    #   Use tf.saved_model.loader.load to load the model and weights\n",
        "    vgg_tag = 'vgg16'\n",
        "    vgg_input_tensor_name = 'image_input:0'\n",
        "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
        "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
        "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
        "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
        "\n",
        "    graph = tf.get_default_graph()\n",
        "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
        "    input = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
        "    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
        "    layer3 = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
        "    layer4 = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
        "    layer7 = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
        "    return input, keep_prob, layer3, layer4, layer7\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
        "    \"\"\"\n",
        "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
        "    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output\n",
        "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
        "    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output\n",
        "    :param num_classes: Number of classes to classify\n",
        "    :return: The Tensor for the last layer of output\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "    l7_conv = tf.layers.conv2d(vgg_layer7_out, num_classes, 1, 1,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                       kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_1',activation = tf.nn.relu)\n",
        "\n",
        "\n",
        "    conv1 = tf.layers.conv2d_transpose(l7_conv, num_classes, 4, 2,\n",
        "                                        padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                        kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_2',activation = tf.nn.relu)\n",
        "\n",
        "    l4_conv = tf.layers.conv2d(vgg_layer4_out, num_classes, 1, 1,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                       kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_3',activation = tf.nn.relu)\n",
        "\n",
        "    skip_1 = tf.add(conv1, l4_conv, name='conv_1_1_4')\n",
        "    #output = tf.layers.batch_normalization(output)\n",
        "    #output = keras.layers.UpSampling2D(size=(2,2),data_format=None,interpolation='bilinear')(output)\n",
        "\n",
        "    conv2 = tf.layers.conv2d_transpose(skip_1, num_classes, 4, 2,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                        kernel_regularizer=tf.keras.regularizers.L2(0.01),  name='conv_1_1_5',activation = tf.nn.relu)\n",
        "    l3_conv = tf.layers.conv2d(vgg_layer3_out, num_classes, 1, 1,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                       kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_6',activation = tf.nn.relu)\n",
        "    skip_3 = tf.add(conv2, l3_conv,  name='conv_1_1_7')\n",
        "\n",
        "    output = tf.layers.conv2d_transpose(skip_3, num_classes, 16, 8,\n",
        "                                        padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                        kernel_regularizer=tf.keras.regularizers.L2(0.01),  name='conv_1_1_8',activation = tf.nn.relu)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
        "    \"\"\"\n",
        "    Build the TensorFLow loss and optimizer operations.\n",
        "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
        "    :param correct_label: TF Placeholder for the correct label image\n",
        "    :param learning_rate: TF Placeholder for the learning rate\n",
        "    :param num_classes: Number of classes to classify\n",
        "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "\n",
        "    logits = tf.reshape(nn_last_layer, (-1, num_classes), name='logits')\n",
        "    labels = tf.reshape(correct_label, (-1, num_classes), name='labels')\n",
        "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(cross_entropy_loss)\n",
        "\n",
        "    return logits, train_op, cross_entropy_loss\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
        "                 correct_label, keep_prob, learning_rate, X_train, y_train, label_values, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Train neural network and print out the loss during training.\n",
        "    :param sess: TF Session\n",
        "    :param epochs: Number of epochs\n",
        "    :param batch_size: Batch size\n",
        "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
        "    :param train_op: TF Operation to train the neural network\n",
        "    :param cross_entropy_loss: TF Tensor for the amount of loss\n",
        "    :param input_image: TF Placeholder for input images\n",
        "    :param correct_label: TF Placeholder for label images\n",
        "    :param keep_prob: TF Placeholder for dropout keep probability\n",
        "    :param learning_rate: TF Placeholder for learning rate\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print('epoch : ', epoch)\n",
        "        for image, targets in get_batches_fn(X_train, y_train, label_values, batch_size):\n",
        "            _, loss = sess.run([train_op, cross_entropy_loss],\n",
        "                               feed_dict={input_image: image, correct_label: targets, keep_prob: KEEP_PROB,\n",
        "                                          learning_rate: LEARNING_RATE})\n",
        "        # Print data on the learning process\n",
        "\n",
        "            print(\"Epoch: {}\".format(epoch + 1), \"/ {}\".format(epochs), \" Loss: {:.3f}\".format(loss))\n",
        "\n",
        "        mean_loss = []\n",
        "        for image, targets in get_batches_fn(X_val, y_val, label_values, batch_size, is_train=False):\n",
        "            loss = sess.run([cross_entropy_loss],\n",
        "                            feed_dict={input_image: image, correct_label: targets, keep_prob: 1})\n",
        "            mean_loss.append(loss)\n",
        "\n",
        "        mean_loss_ = np.mean(np.array(mean_loss))\n",
        "        print(\"Epoch: {}\".format(epoch + 1), \"/ {}\".format(epochs), \" Validation Loss: {:.3f}\".format(mean_loss_))\n",
        "        saver = tf.train.Saver()\n",
        "        print('saving model')\n",
        "        saver.save(sess, './model')\n",
        "\n",
        "\n",
        "#tests.test_train_nn(train_nn)\n",
        "\n",
        "\n",
        "def run():\n",
        "    num_classes = NUM_CLASSES\n",
        "    image_shape = (512, 256)  # Cityscapes dataset should be scaled (Using GTX-1080TI)\n",
        "    data_dir = DATA_DIR\n",
        "    video_dir = VIDEO_DIR\n",
        "    runs_dir = RUNS_DIR\n",
        "\n",
        "    epochs = EPOCHS\n",
        "    batch_size = BATCH_SIZE\n",
        "    is_train = IS_TRAIN\n",
        "    # Download pretrained vgg model\n",
        "    maybe_download_pretrained_vgg(data_dir)\n",
        "    label_values = get_label_info()\n",
        "\n",
        "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
        "    #  https://www.cityscapes-dataset.com/\n",
        "\n",
        "    print('run')\n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session() as sess:\n",
        "        # Path to vgg model\n",
        "        vgg_path = os.path.join(data_dir, 'vgg')\n",
        "        # Create function to get batches\n",
        "        get_batches_fn = gen_batch_function\n",
        "\n",
        "        input, keep_prob, layer3, layer4, layer7 = load_vgg(sess, vgg_path)\n",
        "        print(input)\n",
        "        output = layers(layer3, layer4, layer7, num_classes)\n",
        "        correct_label = tf.placeholder(dtype=tf.float32, shape=(None, None, None, num_classes))\n",
        "        learning_rate = tf.placeholder(dtype=tf.float32)\n",
        "        logits, train_op, cross_entropy_loss = optimize(output, correct_label, learning_rate, num_classes)\n",
        "        logits = tf.nn.softmax(logits, name='softmax')\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()  # Simple model saver\n",
        "\n",
        "        if not is_train:\n",
        "            saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "            save_inference_samples(runs_dir, video_dir, sess, image_shape, logits, keep_prob, input, label_values)\n",
        "        else:\n",
        "            X_train, y_train, X_val, y_val = get_data(data_dir)\n",
        "            train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input, correct_label,\n",
        "                         keep_prob, learning_rate, X_train, y_train, label_values, X_val, y_val)\n",
        "            save_inference_samples(runs_dir, video_dir, sess, image_shape, logits, keep_prob, input,\n",
        "                                                     label_values)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIveBTfBWxXQ",
        "outputId": "2a8a3baf-683e-4e1f-97d4-aa37d1812a1c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.8.0\n",
            "Default GPU Device: /device:GPU:0\n",
            "run\n",
            "INFO:tensorflow:Restoring parameters from /content/train/vgg/variables/variables\n",
            "Tensor(\"image_input:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:575: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:1736: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading X_Train..\n",
            "Loading Y_Train..\n",
            "Loading X_Validation..\n",
            "Loading Y_Validation..\n",
            "epoch :  0\n",
            "Epoch: 1 / 60  Validation Loss: nan\n",
            "saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  1\n",
            "Epoch: 2 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  2\n",
            "Epoch: 3 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  3\n",
            "Epoch: 4 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  4\n",
            "Epoch: 5 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  5\n",
            "Epoch: 6 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  6\n",
            "Epoch: 7 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  7\n",
            "Epoch: 8 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  8\n",
            "Epoch: 9 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  9\n",
            "Epoch: 10 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  10\n",
            "Epoch: 11 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  11\n",
            "Epoch: 12 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  12\n",
            "Epoch: 13 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  13\n",
            "Epoch: 14 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  14\n",
            "Epoch: 15 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  15\n",
            "Epoch: 16 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  16\n",
            "Epoch: 17 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  17\n",
            "Epoch: 18 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  18\n",
            "Epoch: 19 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  19\n",
            "Epoch: 20 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  20\n",
            "Epoch: 21 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  21\n",
            "Epoch: 22 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  22\n",
            "Epoch: 23 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  23\n",
            "Epoch: 24 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  24\n",
            "Epoch: 25 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  25\n",
            "Epoch: 26 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  26\n",
            "Epoch: 27 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  27\n",
            "Epoch: 28 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  28\n",
            "Epoch: 29 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  29\n",
            "Epoch: 30 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  30\n",
            "Epoch: 31 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  31\n",
            "Epoch: 32 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  32\n",
            "Epoch: 33 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  33\n",
            "Epoch: 34 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  34\n",
            "Epoch: 35 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  35\n",
            "Epoch: 36 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  36\n",
            "Epoch: 37 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  37\n",
            "Epoch: 38 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  38\n",
            "Epoch: 39 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  39\n",
            "Epoch: 40 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  40\n",
            "Epoch: 41 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  41\n",
            "Epoch: 42 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  42\n",
            "Epoch: 43 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  43\n",
            "Epoch: 44 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  44\n",
            "Epoch: 45 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  45\n",
            "Epoch: 46 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  46\n",
            "Epoch: 47 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  47\n",
            "Epoch: 48 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  48\n",
            "Epoch: 49 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  49\n",
            "Epoch: 50 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  50\n",
            "Epoch: 51 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  51\n",
            "Epoch: 52 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  52\n",
            "Epoch: 53 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  53\n",
            "Epoch: 54 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  54\n",
            "Epoch: 55 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  55\n",
            "Epoch: 56 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  56\n",
            "Epoch: 57 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  57\n",
            "Epoch: 58 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  58\n",
            "Epoch: 59 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  59\n",
            "Epoch: 60 / 60  Validation Loss: nan\n",
            "saving model\n",
            "Saving test images to: runs/1650911934.1061327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_pCI_zmA8-v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os.path\n",
        "import helper\n",
        "import warnings\n",
        "from distutils.version import LooseVersion\n",
        "import numpy as np\n",
        "import argparse as parser\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "import cv2\n",
        "\n",
        "def load_image(path):\n",
        "    image = cv2.cvtColor(cv2.imread(path, -1), cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "\n",
        "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
        "    \"\"\"\n",
        "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
        "    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output\n",
        "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
        "    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output\n",
        "    :param num_classes: Number of classes to classify\n",
        "    :return: The Tensor for the last layer of output\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "    l7_conv = tf.layers.conv2d(vgg_layer7_out, num_classes, 1, 1,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                       kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_1',activation = tf.nn.relu)\n",
        "\n",
        "\n",
        "    conv1 = tf.layers.conv2d_transpose(l7_conv, num_classes, 4, 2,\n",
        "                                        padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                        kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_2',activation = tf.nn.relu)\n",
        "\n",
        "    l4_conv = tf.layers.conv2d(vgg_layer4_out, num_classes, 1, 1,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                       kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_3',activation = tf.nn.relu)\n",
        "\n",
        "    skip_1 = tf.add(conv1, l4_conv, name='conv_1_1_4')\n",
        "    #output = tf.layers.batch_normalization(output)\n",
        "    #output = keras.layers.UpSampling2D(size=(2,2),data_format=None,interpolation='bilinear')(output)\n",
        "\n",
        "    conv2 = tf.layers.conv2d_transpose(skip_1, num_classes, 4, 2,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                        kernel_regularizer=tf.keras.regularizers.L2(0.01),  name='conv_1_1_5',activation = tf.nn.relu)\n",
        "    l3_conv = tf.layers.conv2d(vgg_layer3_out, num_classes, 1, 1,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                       kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_6',activation = tf.nn.relu)\n",
        "    skip_3 = tf.add(conv2, l3_conv,  name='conv_1_1_7')\n",
        "\n",
        "    output = tf.layers.conv2d_transpose(skip_3, num_classes, 16, 8,\n",
        "                                        padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                        kernel_regularizer=tf.keras.regularizers.L2(0.01),  name='conv_1_1_8',activation = tf.nn.relu)\n",
        "\n",
        "    return output\n",
        "\n",
        "def load_vgg(sess, vgg_path):\n",
        "    \"\"\"\n",
        "    Load Pretrained VGG Model into TensorFlow.\n",
        "    :param sess: TensorFlow Session\n",
        "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
        "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "    #   Use tf.saved_model.loader.load to load the model and weights\n",
        "    vgg_tag = 'vgg16'\n",
        "    vgg_input_tensor_name = 'image_input:0'\n",
        "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
        "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
        "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
        "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
        "\n",
        "    graph = tf.get_default_graph()\n",
        "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
        "    input = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
        "    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
        "    layer3 = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
        "    layer4 = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
        "    layer7 = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
        "    return input, keep_prob, layer3, layer4, layer7\n",
        "\n",
        "\n",
        "def pipeline_final(img, is_video):\n",
        "    channel = 1 if is_video else 4\n",
        "\n",
        "    img = cv2.resize(img, dsize=(512, 256))\n",
        "    img = np.array([img])\n",
        "    softmax_   = sess.run([softmax],\n",
        "                               feed_dict={input: img, keep_prob: 1})\n",
        "    logits_ = (softmax_[0].reshape(1, 256, 512, 29))\n",
        "    output_image = reverse_one_hot(logits_[0])\n",
        "\n",
        "    print(output_image.shape)\n",
        "\n",
        "    out_vis_image = colour_code_segmentation(output_image, label_values)\n",
        "\n",
        "    a = cv2.cvtColor(np.uint8(out_vis_image), channel)\n",
        "\n",
        "    b = cv2.cvtColor(np.uint8(img[0]), channel)\n",
        "\n",
        "    added_image = cv2.addWeighted(a, 1, b, 1, channel)\n",
        "    added_image = cv2.resize(added_image, dsize=(512, 256))\n",
        "\n",
        "    return added_image\n",
        "\n",
        "def pipeline_video(img):\n",
        "    return pipeline_final(img, True)\n",
        "\n",
        "def pipeline_img(img):\n",
        "    return pipeline_final(img, False)\n",
        "\n",
        "def process(media_dir, save_dir):\n",
        "    global sess, softmax, label_values, input, keep_prob\n",
        "\n",
        "    data_dir = '/content/train'\n",
        "    num_classes = 29\n",
        "\n",
        "    label_values = get_label_info()\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "\n",
        "    # Path to vgg model\n",
        "    vgg_path = os.path.join(data_dir, 'vgg')\n",
        "    # Create function to get batches\n",
        "\n",
        "    input, keep_prob, layer3, layer4, layer7 = load_vgg(sess, vgg_path)\n",
        "    output = layers(layer3, layer4, layer7, num_classes)\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    saver = tf.train.Saver()  # Simple model saver\n",
        "\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "\n",
        "    logits = tf.reshape(output, (-1, num_classes))\n",
        "    softmax = tf.nn.softmax(logits, name='softmax')\n",
        "\n",
        "\n",
        "    try:\n",
        "        img = load_image(media_dir)\n",
        "        output = os.path.join(save_dir, 'output_image.png')\n",
        "        img = pipeline_img(img)\n",
        "        cv2.imwrite(output, img)\n",
        "    except Exception as ex:\n",
        "        output = os.path.join(save_dir, 'output_video.mp4')\n",
        "        clip1 = VideoFileClip(media_dir)\n",
        "        white_clip = clip1.fl_image(pipeline_video)\n",
        "        white_clip.write_videofile(output, audio=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "\n",
        "        media_dir = '/content/train/10.jpg'\n",
        "        save_dir = '/content/saved_op'\n",
        "        \n",
        "\n",
        "        crawler = process(media_dir= media_dir, save_dir= save_dir )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1s7a146f17W",
        "outputId": "1bf62d2a-12e3-4288-8041-db5658e912bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/train/vgg/variables/variables\n",
            "INFO:tensorflow:Restoring parameters from ./model\n",
            "(256, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('/content/saved_op')"
      ],
      "metadata": {
        "id": "RMO-R8hJtd2-"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}