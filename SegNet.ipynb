{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SegNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPfCbi3RLi6fDdSGInpG9qJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saptarshidatta96/MTech_Sem3/blob/main/SegNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/divamgupta/image-segmentation-keras"
      ],
      "metadata": {
        "id": "2rHQnJ-WRwb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models as sm\n",
        "\n",
        "sm.set_framework('tf.keras')\n",
        "\n",
        "sm.framework()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VdiP_RZPWgsy",
        "outputId": "db56fd46-8d2d-438a-d897-198f5c30a62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tf.keras'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_ORDERING_CHANNELS_LAST = \"channels_last\"\n",
        "IMAGE_ORDERING_CHANNELS_FIRST = \"channels_first\"\n",
        "\n",
        "# Default IMAGE_ORDERING = channels_last\n",
        "IMAGE_ORDERING = IMAGE_ORDERING_CHANNELS_LAST"
      ],
      "metadata": {
        "id": "6RFQl-qKUOMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "\n",
        "def vanilla_encoder(input_height=224,  input_width=224, channels=3):\n",
        "\n",
        "    kernel = 3\n",
        "    filter_size = 64\n",
        "    pad = 1\n",
        "    pool_size = 2\n",
        "\n",
        "    if IMAGE_ORDERING == 'channels_first':\n",
        "        img_input = Input(shape=(channels, input_height, input_width))\n",
        "    elif IMAGE_ORDERING == 'channels_last':\n",
        "        img_input = Input(shape=(input_height, input_width, channels))\n",
        "\n",
        "    x = img_input\n",
        "    levels = []\n",
        "\n",
        "    x = (ZeroPadding2D((pad, pad), data_format=IMAGE_ORDERING))(x)\n",
        "    x = (Conv2D(filter_size, (kernel, kernel),\n",
        "                data_format=IMAGE_ORDERING, padding='valid'))(x)\n",
        "    x = (BatchNormalization())(x)\n",
        "    x = (Activation('relu'))(x)\n",
        "    x = (MaxPooling2D((pool_size, pool_size), data_format=IMAGE_ORDERING))(x)\n",
        "    levels.append(x)\n",
        "\n",
        "    x = (ZeroPadding2D((pad, pad), data_format=IMAGE_ORDERING))(x)\n",
        "    x = (Conv2D(128, (kernel, kernel), data_format=IMAGE_ORDERING,\n",
        "         padding='valid'))(x)\n",
        "    x = (BatchNormalization())(x)\n",
        "    x = (Activation('relu'))(x)\n",
        "    x = (MaxPooling2D((pool_size, pool_size), data_format=IMAGE_ORDERING))(x)\n",
        "    levels.append(x)\n",
        "\n",
        "    for _ in range(3):\n",
        "        x = (ZeroPadding2D((pad, pad), data_format=IMAGE_ORDERING))(x)\n",
        "        x = (Conv2D(256, (kernel, kernel),\n",
        "                    data_format=IMAGE_ORDERING, padding='valid'))(x)\n",
        "        x = (BatchNormalization())(x)\n",
        "        x = (Activation('relu'))(x)\n",
        "        x = (MaxPooling2D((pool_size, pool_size),\n",
        "             data_format=IMAGE_ORDERING))(x)\n",
        "        levels.append(x)\n",
        "\n",
        "    return img_input, levels"
      ],
      "metadata": {
        "id": "cGYro8QKV_Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "import keras.backend as K\n",
        "import keras\n",
        "\n",
        "\n",
        "\n",
        "BASE_WEIGHT_PATH = ('https://github.com/fchollet/deep-learning-models/'\n",
        "                    'releases/download/v0.6/')\n",
        "\n",
        "\n",
        "def relu6(x):\n",
        "    return K.relu(x, max_value=6)\n",
        "\n",
        "\n",
        "def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n",
        "\n",
        "    channel_axis = 1 if IMAGE_ORDERING == 'channels_first' else -1\n",
        "    filters = int(filters * alpha)\n",
        "    x = ZeroPadding2D(padding=(1, 1), name='conv1_pad',\n",
        "                      data_format=IMAGE_ORDERING)(inputs)\n",
        "    x = Conv2D(filters, kernel, data_format=IMAGE_ORDERING,\n",
        "               padding='valid',\n",
        "               use_bias=False,\n",
        "               strides=strides,\n",
        "               name='conv1')(x)\n",
        "    x = BatchNormalization(axis=channel_axis, name='conv1_bn')(x)\n",
        "    return Activation(relu6, name='conv1_relu')(x)\n",
        "\n",
        "\n",
        "def _depthwise_conv_block(inputs, pointwise_conv_filters, alpha,\n",
        "                          depth_multiplier=1, strides=(1, 1), block_id=1):\n",
        "\n",
        "    channel_axis = 1 if IMAGE_ORDERING == 'channels_first' else -1\n",
        "    pointwise_conv_filters = int(pointwise_conv_filters * alpha)\n",
        "\n",
        "    x = ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING,\n",
        "                      name='conv_pad_%d' % block_id)(inputs)\n",
        "    x = DepthwiseConv2D((3, 3), data_format=IMAGE_ORDERING,\n",
        "                        padding='valid',\n",
        "                        depth_multiplier=depth_multiplier,\n",
        "                        strides=strides,\n",
        "                        use_bias=False,\n",
        "                        name='conv_dw_%d' % block_id)(x)\n",
        "    x = BatchNormalization(\n",
        "        axis=channel_axis, name='conv_dw_%d_bn' % block_id)(x)\n",
        "    x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)\n",
        "\n",
        "    x = Conv2D(pointwise_conv_filters, (1, 1), data_format=IMAGE_ORDERING,\n",
        "               padding='same',\n",
        "               use_bias=False,\n",
        "               strides=(1, 1),\n",
        "               name='conv_pw_%d' % block_id)(x)\n",
        "    x = BatchNormalization(axis=channel_axis,\n",
        "                           name='conv_pw_%d_bn' % block_id)(x)\n",
        "    return Activation(relu6, name='conv_pw_%d_relu' % block_id)(x)\n",
        "\n",
        "\n",
        "def get_mobilenet_encoder(input_height=224, input_width=224,\n",
        "                          pretrained='imagenet', channels=3):\n",
        "\n",
        "    # todo add more alpha and stuff\n",
        "\n",
        "    assert (K.image_data_format() ==\n",
        "            'channels_last'), \"Currently only channels last mode is supported\"\n",
        "    assert (IMAGE_ORDERING ==\n",
        "            'channels_last'), \"Currently only channels last mode is supported\"\n",
        "\n",
        "    assert input_height % 32 == 0\n",
        "    assert input_width % 32 == 0\n",
        "\n",
        "    alpha = 1.0\n",
        "    depth_multiplier = 1\n",
        "    dropout = 1e-3\n",
        "\n",
        "    img_input = Input(shape=(input_height, input_width, channels))\n",
        "\n",
        "    x = _conv_block(img_input, 32, alpha, strides=(2, 2))\n",
        "    x = _depthwise_conv_block(x, 64, alpha, depth_multiplier, block_id=1)\n",
        "    f1 = x\n",
        "\n",
        "    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier,\n",
        "                              strides=(2, 2), block_id=2)\n",
        "    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, block_id=3)\n",
        "    f2 = x\n",
        "\n",
        "    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier,\n",
        "                              strides=(2, 2), block_id=4)\n",
        "    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, block_id=5)\n",
        "    f3 = x\n",
        "\n",
        "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier,\n",
        "                              strides=(2, 2), block_id=6)\n",
        "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=7)\n",
        "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=8)\n",
        "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=9)\n",
        "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=10)\n",
        "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=11)\n",
        "    f4 = x\n",
        "\n",
        "    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier,\n",
        "                              strides=(2, 2), block_id=12)\n",
        "    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, block_id=13)\n",
        "    f5 = x\n",
        "\n",
        "    if pretrained == 'imagenet':\n",
        "        model_name = 'mobilenet_%s_%d_tf_no_top.h5' % ('1_0', 224)\n",
        "\n",
        "        weight_path = BASE_WEIGHT_PATH + model_name\n",
        "        weights_path = keras.utils.get_file(model_name, weight_path)\n",
        "\n",
        "        Model(img_input, x).load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "\n",
        "    return img_input, [f1, f2, f3, f4, f5]"
      ],
      "metadata": {
        "id": "PprZTk-WV1yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "\n",
        "\n",
        "\n",
        "if IMAGE_ORDERING == 'channels_first':\n",
        "    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n",
        "                     \"releases/download/v0.1/\" \\\n",
        "                     \"vgg16_weights_th_dim_ordering_th_kernels_notop.h5\"\n",
        "elif IMAGE_ORDERING == 'channels_last':\n",
        "    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n",
        "                     \"releases/download/v0.1/\" \\\n",
        "                     \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "\n",
        "\n",
        "def get_vgg_encoder(input_height=224,  input_width=224, pretrained='imagenet', channels=3):\n",
        "\n",
        "    assert input_height % 32 == 0\n",
        "    assert input_width % 32 == 0\n",
        "\n",
        "    if IMAGE_ORDERING == 'channels_first':\n",
        "        img_input = Input(shape=(channels, input_height, input_width))\n",
        "    elif IMAGE_ORDERING == 'channels_last':\n",
        "        img_input = Input(shape=(input_height, input_width, channels))\n",
        "\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "               name='block1_conv1', data_format=IMAGE_ORDERING)(img_input)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
        "               name='block1_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    f1 = x\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n",
        "               name='block2_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n",
        "               name='block2_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    f2 = x\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
        "               name='block3_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
        "               name='block3_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
        "               name='block3_conv3', data_format=IMAGE_ORDERING)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    f3 = x\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block4_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block4_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block4_conv3', data_format=IMAGE_ORDERING)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    f4 = x\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block5_conv1', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block5_conv2', data_format=IMAGE_ORDERING)(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
        "               name='block5_conv3', data_format=IMAGE_ORDERING)(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool',\n",
        "                     data_format=IMAGE_ORDERING)(x)\n",
        "    f5 = x\n",
        "\n",
        "    if pretrained == 'imagenet':\n",
        "        VGG_Weights_path = keras.utils.get_file(\n",
        "            pretrained_url.split(\"/\")[-1], pretrained_url)\n",
        "        Model(img_input, x).load_weights(VGG_Weights_path, by_name=True, skip_mismatch=True)\n",
        "\n",
        "    return img_input, [f1, f2, f3, f4, f5]"
      ],
      "metadata": {
        "id": "KskkTbfnVRAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import imgaug as ia\n",
        "    from imgaug import augmenters as iaa\n",
        "except ImportError:\n",
        "    print(\"Error in loading augmentation, can't import imgaug.\"\n",
        "          \"Please make sure it is installed.\")\n",
        "\n",
        "\n",
        "IMAGE_AUGMENTATION_SEQUENCE = None\n",
        "IMAGE_AUGMENTATION_NUM_TRIES = 10\n",
        "\n",
        "loaded_augmentation_name = \"\"\n",
        "\n",
        "\n",
        "def _load_augmentation_aug_geometric():\n",
        "    return iaa.OneOf([\n",
        "        iaa.Sequential([iaa.Fliplr(0.5), iaa.Flipud(0.2)]),\n",
        "        iaa.CropAndPad(percent=(-0.05, 0.1),\n",
        "                       pad_mode='constant',\n",
        "                       pad_cval=(0, 255)),\n",
        "        iaa.Crop(percent=(0.0, 0.1)),\n",
        "        iaa.Crop(percent=(0.3, 0.5)),\n",
        "        iaa.Crop(percent=(0.3, 0.5)),\n",
        "        iaa.Crop(percent=(0.3, 0.5)),\n",
        "        iaa.Sequential([\n",
        "            iaa.Affine(\n",
        "                    # scale images to 80-120% of their size,\n",
        "                    # individually per axis\n",
        "                    scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
        "                    # translate by -20 to +20 percent (per axis)\n",
        "                    translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "                    rotate=(-45, 45),  # rotate by -45 to +45 degrees\n",
        "                    shear=(-16, 16),  # shear by -16 to +16 degrees\n",
        "                    # use nearest neighbour or bilinear interpolation (fast)\n",
        "                    order=[0, 1],\n",
        "                    # if mode is constant, use a cval between 0 and 255\n",
        "                    mode='constant',\n",
        "                    cval=(0, 255),\n",
        "                    # use any of scikit-image's warping modes\n",
        "                    # (see 2nd image from the top for examples)\n",
        "            ),\n",
        "            iaa.Sometimes(0.3, iaa.Crop(percent=(0.3, 0.5)))])\n",
        "    ])\n",
        "\n",
        "\n",
        "def _load_augmentation_aug_non_geometric():\n",
        "    return iaa.Sequential([\n",
        "        iaa.Sometimes(0.3, iaa.Multiply((0.5, 1.5), per_channel=0.5)),\n",
        "        iaa.Sometimes(0.2, iaa.JpegCompression(compression=(70, 99))),\n",
        "        iaa.Sometimes(0.2, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
        "        iaa.Sometimes(0.2, iaa.MotionBlur(k=15, angle=[-45, 45])),\n",
        "        iaa.Sometimes(0.2, iaa.MultiplyHue((0.5, 1.5))),\n",
        "        iaa.Sometimes(0.2, iaa.MultiplySaturation((0.5, 1.5))),\n",
        "        iaa.Sometimes(0.34, iaa.MultiplyHueAndSaturation((0.5, 1.5),\n",
        "                                                         per_channel=True)),\n",
        "        iaa.Sometimes(0.34, iaa.Grayscale(alpha=(0.0, 1.0))),\n",
        "        iaa.Sometimes(0.2, iaa.ChangeColorTemperature((1100, 10000))),\n",
        "        iaa.Sometimes(0.1, iaa.GammaContrast((0.5, 2.0))),\n",
        "        iaa.Sometimes(0.2, iaa.SigmoidContrast(gain=(3, 10),\n",
        "                                               cutoff=(0.4, 0.6))),\n",
        "        iaa.Sometimes(0.1, iaa.CLAHE()),\n",
        "        iaa.Sometimes(0.1, iaa.HistogramEqualization()),\n",
        "        iaa.Sometimes(0.2, iaa.LinearContrast((0.5, 2.0), per_channel=0.5)),\n",
        "        iaa.Sometimes(0.1, iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)))\n",
        "    ])\n",
        "\n",
        "\n",
        "def _load_augmentation_aug_all2():\n",
        "    return iaa.Sequential([\n",
        "        iaa.Sometimes(0.65, _load_augmentation_aug_non_geometric()),\n",
        "        iaa.Sometimes(0.65, _load_augmentation_aug_geometric())\n",
        "    ])\n",
        "\n",
        "\n",
        "def _load_augmentation_aug_all():\n",
        "    \"\"\" Load image augmentation model \"\"\"\n",
        "\n",
        "    def sometimes(aug):\n",
        "        return iaa.Sometimes(0.5, aug)\n",
        "\n",
        "    return iaa.Sequential(\n",
        "        [\n",
        "            # apply the following augmenters to most images\n",
        "            iaa.Fliplr(0.5),  # horizontally flip 50% of all images\n",
        "            iaa.Flipud(0.2),  # vertically flip 20% of all images\n",
        "            # crop images by -5% to 10% of their height/width\n",
        "            sometimes(iaa.CropAndPad(\n",
        "                percent=(-0.05, 0.1),\n",
        "                pad_mode='constant',\n",
        "                pad_cval=(0, 255)\n",
        "            )),\n",
        "            sometimes(iaa.Affine(\n",
        "                # scale images to 80-120% of their size, individually per axis\n",
        "                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
        "                # translate by -20 to +20 percent (per axis)\n",
        "                translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "                rotate=(-45, 45),  # rotate by -45 to +45 degrees\n",
        "                shear=(-16, 16),  # shear by -16 to +16 degrees\n",
        "                # use nearest neighbour or bilinear interpolation (fast)\n",
        "                order=[0, 1],\n",
        "                # if mode is constant, use a cval between 0 and 255\n",
        "                cval=(0, 255),\n",
        "                # use any of scikit-image's warping modes\n",
        "                # (see 2nd image from the top for examples)\n",
        "                mode='constant'\n",
        "            )),\n",
        "            # execute 0 to 5 of the following (less important) augmenters per\n",
        "            # image don't execute all of them, as that would often be way too\n",
        "            # strong\n",
        "            iaa.SomeOf((0, 5),\n",
        "                       [\n",
        "                # convert images into their superpixel representation\n",
        "                sometimes(iaa.Superpixels(\n",
        "                    p_replace=(0, 1.0), n_segments=(20, 200))),\n",
        "                iaa.OneOf([\n",
        "                    # blur images with a sigma between 0 and 3.0\n",
        "                    iaa.GaussianBlur((0, 3.0)),\n",
        "                    # blur image using local means with kernel sizes\n",
        "                    # between 2 and 7\n",
        "                    iaa.AverageBlur(k=(2, 7)),\n",
        "                    # blur image using local medians with kernel sizes\n",
        "                    # between 2 and 7\n",
        "                    iaa.MedianBlur(k=(3, 11)),\n",
        "                ]),\n",
        "                iaa.Sharpen(alpha=(0, 1.0), lightness=(\n",
        "                            0.75, 1.5)),  # sharpen images\n",
        "                iaa.Emboss(alpha=(0, 1.0), strength=(\n",
        "                    0, 2.0)),  # emboss images\n",
        "                # search either for all edges or for directed edges,\n",
        "                # blend the result with the original image using a blobby mask\n",
        "                iaa.BlendAlphaSimplexNoise(iaa.OneOf([\n",
        "                    iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
        "                    iaa.DirectedEdgeDetect(\n",
        "                        alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
        "                ])),\n",
        "                # add gaussian noise to images\n",
        "                iaa.AdditiveGaussianNoise(loc=0, scale=(\n",
        "                    0.0, 0.05*255), per_channel=0.5),\n",
        "                iaa.OneOf([\n",
        "                    # randomly remove up to 10% of the pixels\n",
        "                    iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
        "                    iaa.CoarseDropout((0.03, 0.15), size_percent=(\n",
        "                        0.02, 0.05), per_channel=0.2),\n",
        "                ]),\n",
        "                # invert color channels\n",
        "                iaa.Invert(0.05, per_channel=True),\n",
        "                # change brightness of images (by -10 to 10 of original value)\n",
        "                iaa.Add((-10, 10), per_channel=0.5),\n",
        "                # change hue and saturation\n",
        "                iaa.AddToHueAndSaturation((-20, 20)),\n",
        "                # either change the brightness of the whole image (sometimes\n",
        "                # per channel) or change the brightness of subareas\n",
        "                iaa.OneOf([\n",
        "                    iaa.Multiply(\n",
        "                                (0.5, 1.5), per_channel=0.5),\n",
        "                    iaa.BlendAlphaFrequencyNoise(\n",
        "                        exponent=(-4, 0),\n",
        "                        foreground=iaa.Multiply(\n",
        "                            (0.5, 1.5), per_channel=True),\n",
        "                        background=iaa.contrast.LinearContrast(\n",
        "                            (0.5, 2.0))\n",
        "                    )\n",
        "                ]),\n",
        "                # improve or worsen the contrast\n",
        "                iaa.contrast.LinearContrast((0.5, 2.0), per_channel=0.5),\n",
        "                iaa.Grayscale(alpha=(0.0, 1.0)),\n",
        "                # move pixels locally around (with random strengths)\n",
        "                sometimes(iaa.ElasticTransformation(\n",
        "                    alpha=(0.5, 3.5), sigma=0.25)),\n",
        "                # sometimes move parts of the image around\n",
        "                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))),\n",
        "                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
        "            ],\n",
        "                random_order=True\n",
        "            )\n",
        "        ],\n",
        "        random_order=True\n",
        "    )\n",
        "\n",
        "\n",
        "augmentation_functions = {\n",
        "    \"aug_all\": _load_augmentation_aug_all,\n",
        "    \"aug_all2\": _load_augmentation_aug_all2,\n",
        "    \"aug_geometric\": _load_augmentation_aug_geometric,\n",
        "    \"aug_non_geometric\": _load_augmentation_aug_non_geometric\n",
        "}\n",
        "\n",
        "\n",
        "def _load_augmentation(augmentation_name=\"aug_all\"):\n",
        "\n",
        "    global IMAGE_AUGMENTATION_SEQUENCE\n",
        "\n",
        "    if augmentation_name not in augmentation_functions:\n",
        "        raise ValueError(\"Augmentation name not supported\")\n",
        "\n",
        "    IMAGE_AUGMENTATION_SEQUENCE = augmentation_functions[augmentation_name]()\n",
        "\n",
        "\n",
        "def _augment_seg(img, seg, augmentation_name=\"aug_all\", other_imgs=None):\n",
        "\n",
        "    global loaded_augmentation_name\n",
        "\n",
        "    if (not IMAGE_AUGMENTATION_SEQUENCE) or\\\n",
        "       (augmentation_name != loaded_augmentation_name):\n",
        "        _load_augmentation(augmentation_name)\n",
        "        loaded_augmentation_name = augmentation_name\n",
        "\n",
        "    # Create a deterministic augmentation from the random one\n",
        "    aug_det = IMAGE_AUGMENTATION_SEQUENCE.to_deterministic()\n",
        "    # Augment the input image\n",
        "    image_aug = aug_det.augment_image(img)\n",
        "\n",
        "    if other_imgs is not None:\n",
        "        image_aug = [image_aug]\n",
        "\n",
        "        for other_img in other_imgs:\n",
        "            image_aug.append(aug_det.augment_image(other_img))\n",
        "\n",
        "    segmap = ia.SegmentationMapsOnImage(\n",
        "        seg, shape=img.shape)\n",
        "    segmap_aug = aug_det.augment_segmentation_maps(segmap)\n",
        "    segmap_aug = segmap_aug.get_arr()\n",
        "\n",
        "    return image_aug, segmap_aug\n",
        "\n",
        "\n",
        "def _custom_augment_seg(img, seg, augmentation_function, other_imgs=None):\n",
        "    augmentation_functions['custom_aug'] = augmentation_function\n",
        "\n",
        "    return _augment_seg(img, seg, \"custom_aug\", other_imgs=other_imgs)\n",
        "\n",
        "\n",
        "def _try_n_times(fn, n, *args, **kargs):\n",
        "    \"\"\" Try a function N times \"\"\"\n",
        "    attempts = 0\n",
        "    while attempts < n:\n",
        "        try:\n",
        "            return fn(*args, **kargs)\n",
        "        except Exception:\n",
        "            attempts += 1\n",
        "\n",
        "    return fn(*args, **kargs)\n",
        "\n",
        "\n",
        "def augment_seg(img, seg, augmentation_name=\"aug_all\", other_imgs=None):\n",
        "    return _try_n_times(_augment_seg, IMAGE_AUGMENTATION_NUM_TRIES,\n",
        "                        img, seg, augmentation_name=augmentation_name,\n",
        "                        other_imgs=other_imgs)\n",
        "\n",
        "\n",
        "def custom_augment_seg(img, seg, augmentation_function, other_imgs=None):\n",
        "    return _try_n_times(_custom_augment_seg, IMAGE_AUGMENTATION_NUM_TRIES,\n",
        "                        img, seg, augmentation_function=augmentation_function,\n",
        "                        other_imgs=other_imgs)"
      ],
      "metadata": {
        "id": "VCm7WJx4Xi1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import six\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "try:\n",
        "    from collections.abc import Sequence\n",
        "except ImportError:\n",
        "    from collections import Sequence\n",
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print(\"tqdm not found, disabling progress bars\")\n",
        "\n",
        "    def tqdm(iter):\n",
        "        return iter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DATA_LOADER_SEED = 0\n",
        "\n",
        "random.seed(DATA_LOADER_SEED)\n",
        "class_colors = [(random.randint(0, 255), random.randint(\n",
        "    0, 255), random.randint(0, 255)) for _ in range(5000)]\n",
        "\n",
        "\n",
        "ACCEPTABLE_IMAGE_FORMATS = [\".jpg\", \".jpeg\", \".png\", \".bmp\"]\n",
        "ACCEPTABLE_SEGMENTATION_FORMATS = [\".png\", \".bmp\"]\n",
        "\n",
        "\n",
        "class DataLoaderError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "def get_image_list_from_path(images_path ):\n",
        "    image_files = []\n",
        "    for dir_entry in os.listdir(images_path):\n",
        "            if os.path.isfile(os.path.join(images_path, dir_entry)) and \\\n",
        "                    os.path.splitext(dir_entry)[1] in ACCEPTABLE_IMAGE_FORMATS:\n",
        "                file_name, file_extension = os.path.splitext(dir_entry)\n",
        "                image_files.append(os.path.join(images_path, dir_entry))\n",
        "    return image_files\n",
        "\n",
        "\n",
        "def get_pairs_from_paths(images_path, segs_path, ignore_non_matching=False, other_inputs_paths=None):\n",
        "    \"\"\" Find all the images from the images_path directory and\n",
        "        the segmentation images from the segs_path directory\n",
        "        while checking integrity of data \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    image_files = []\n",
        "    segmentation_files = {}\n",
        "\n",
        "    for dir_entry in os.listdir(images_path):\n",
        "        if os.path.isfile(os.path.join(images_path, dir_entry)) and \\\n",
        "                os.path.splitext(dir_entry)[1] in ACCEPTABLE_IMAGE_FORMATS:\n",
        "            file_name, file_extension = os.path.splitext(dir_entry)\n",
        "            image_files.append((file_name, file_extension,\n",
        "                                os.path.join(images_path, dir_entry)))\n",
        "\n",
        "    if other_inputs_paths is not None:\n",
        "        other_inputs_files = []\n",
        "\n",
        "        for i, other_inputs_path in enumerate(other_inputs_paths):\n",
        "            temp = []\n",
        "\n",
        "            for y, dir_entry in enumerate(os.listdir(other_inputs_path)):\n",
        "                if os.path.isfile(os.path.join(other_inputs_path, dir_entry)) and \\\n",
        "                        os.path.splitext(dir_entry)[1] in ACCEPTABLE_IMAGE_FORMATS:\n",
        "                    file_name, file_extension = os.path.splitext(dir_entry)\n",
        "\n",
        "                    temp.append((file_name, file_extension,\n",
        "                                 os.path.join(other_inputs_path, dir_entry)))\n",
        "\n",
        "            other_inputs_files.append(temp)\n",
        "\n",
        "    for dir_entry in os.listdir(segs_path):\n",
        "        if os.path.isfile(os.path.join(segs_path, dir_entry)) and \\\n",
        "           os.path.splitext(dir_entry)[1] in ACCEPTABLE_SEGMENTATION_FORMATS:\n",
        "            file_name, file_extension = os.path.splitext(dir_entry)\n",
        "            full_dir_entry = os.path.join(segs_path, dir_entry)\n",
        "            if file_name in segmentation_files:\n",
        "                raise DataLoaderError(\"Segmentation file with filename {0}\"\n",
        "                                      \" already exists and is ambiguous to\"\n",
        "                                      \" resolve with path {1}.\"\n",
        "                                      \" Please remove or rename the latter.\"\n",
        "                                      .format(file_name, full_dir_entry))\n",
        "\n",
        "            segmentation_files[file_name] = (file_extension, full_dir_entry)\n",
        "\n",
        "    return_value = []\n",
        "    # Match the images and segmentations\n",
        "    for image_file, _, image_full_path in image_files:\n",
        "        if image_file in segmentation_files:\n",
        "            if other_inputs_paths is not None:\n",
        "                other_inputs = []\n",
        "                for file_paths in other_inputs_files:\n",
        "                    success = False\n",
        "\n",
        "                    for (other_file, _, other_full_path) in file_paths:\n",
        "                        if image_file == other_file:\n",
        "                            other_inputs.append(other_full_path)\n",
        "                            success = True\n",
        "                            break\n",
        "\n",
        "                    if not success:\n",
        "                        raise ValueError(\"There was no matching other input to\", image_file, \"in directory\")\n",
        "\n",
        "                return_value.append((image_full_path,\n",
        "                                     segmentation_files[image_file][1], other_inputs))\n",
        "            else:\n",
        "                return_value.append((image_full_path,\n",
        "                                     segmentation_files[image_file][1]))\n",
        "        elif ignore_non_matching:\n",
        "            continue\n",
        "        else:\n",
        "            # Error out\n",
        "            raise DataLoaderError(\"No corresponding segmentation \"\n",
        "                                  \"found for image {0}.\"\n",
        "                                  .format(image_full_path))\n",
        "\n",
        "    return return_value\n",
        "\n",
        "\n",
        "def get_image_array(image_input,\n",
        "                    width, height,\n",
        "                    imgNorm=\"sub_mean\", ordering='channels_first', read_image_type=1):\n",
        "    \"\"\" Load image array from input \"\"\"\n",
        "\n",
        "    if type(image_input) is np.ndarray:\n",
        "        # It is already an array, use it as it is\n",
        "        img = image_input\n",
        "    elif isinstance(image_input, six.string_types):\n",
        "        if not os.path.isfile(image_input):\n",
        "            raise DataLoaderError(\"get_image_array: path {0} doesn't exist\"\n",
        "                                  .format(image_input))\n",
        "        img = cv2.imread(image_input, read_image_type)\n",
        "    else:\n",
        "        raise DataLoaderError(\"get_image_array: Can't process input type {0}\"\n",
        "                              .format(str(type(image_input))))\n",
        "\n",
        "    if imgNorm == \"sub_and_divide\":\n",
        "        img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n",
        "    elif imgNorm == \"sub_mean\":\n",
        "        img = cv2.resize(img, (width, height))\n",
        "        img = img.astype(np.float32)\n",
        "        img = np.atleast_3d(img)\n",
        "\n",
        "        means = [103.939, 116.779, 123.68]\n",
        "\n",
        "        for i in range(min(img.shape[2], len(means))):\n",
        "            img[:, :, i] -= means[i]\n",
        "\n",
        "        img = img[:, :, ::-1]\n",
        "    elif imgNorm == \"divide\":\n",
        "        img = cv2.resize(img, (width, height))\n",
        "        img = img.astype(np.float32)\n",
        "        img = img/255.0\n",
        "\n",
        "    if ordering == 'channels_first':\n",
        "        img = np.rollaxis(img, 2, 0)\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_segmentation_array(image_input, nClasses,\n",
        "                           width, height, no_reshape=False, read_image_type=1):\n",
        "    \"\"\" Load segmentation array from input \"\"\"\n",
        "\n",
        "    seg_labels = np.zeros((height, width, nClasses))\n",
        "\n",
        "    if type(image_input) is np.ndarray:\n",
        "        # It is already an array, use it as it is\n",
        "        img = image_input\n",
        "    elif isinstance(image_input, six.string_types):\n",
        "        if not os.path.isfile(image_input):\n",
        "            raise DataLoaderError(\"get_segmentation_array: \"\n",
        "                                  \"path {0} doesn't exist\".format(image_input))\n",
        "        img = cv2.imread(image_input, read_image_type)\n",
        "    else:\n",
        "        raise DataLoaderError(\"get_segmentation_array: \"\n",
        "                              \"Can't process input type {0}\"\n",
        "                              .format(str(type(image_input))))\n",
        "\n",
        "    img = cv2.resize(img, (width, height), interpolation=cv2.INTER_NEAREST)\n",
        "    img = img[:, :, 0]\n",
        "\n",
        "    for c in range(nClasses):\n",
        "        seg_labels[:, :, c] = (img == c).astype(int)\n",
        "\n",
        "    if not no_reshape:\n",
        "        seg_labels = np.reshape(seg_labels, (width*height, nClasses))\n",
        "\n",
        "    return seg_labels\n",
        "\n",
        "\n",
        "def verify_segmentation_dataset(images_path, segs_path,\n",
        "                                n_classes, show_all_errors=False):\n",
        "    try:\n",
        "        img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n",
        "        if not len(img_seg_pairs):\n",
        "            print(\"Couldn't load any data from images_path: \"\n",
        "                  \"{0} and segmentations path: {1}\"\n",
        "                  .format(images_path, segs_path))\n",
        "            return False\n",
        "\n",
        "        return_value = True\n",
        "        for im_fn, seg_fn in tqdm(img_seg_pairs):\n",
        "            img = cv2.imread(im_fn)\n",
        "            seg = cv2.imread(seg_fn)\n",
        "            # Check dimensions match\n",
        "            if not img.shape == seg.shape:\n",
        "                return_value = False\n",
        "                print(\"The size of image {0} and its segmentation {1} \"\n",
        "                      \"doesn't match (possibly the files are corrupt).\"\n",
        "                      .format(im_fn, seg_fn))\n",
        "                if not show_all_errors:\n",
        "                    break\n",
        "            else:\n",
        "                max_pixel_value = np.max(seg[:, :, 0])\n",
        "                if max_pixel_value >= n_classes:\n",
        "                    return_value = False\n",
        "                    print(\"The pixel values of the segmentation image {0} \"\n",
        "                          \"violating range [0, {1}]. \"\n",
        "                          \"Found maximum pixel value {2}\"\n",
        "                          .format(seg_fn, str(n_classes - 1), max_pixel_value))\n",
        "                    if not show_all_errors:\n",
        "                        break\n",
        "        if return_value:\n",
        "            print(\"Dataset verified! \")\n",
        "        else:\n",
        "            print(\"Dataset not verified!\")\n",
        "        return return_value\n",
        "    except DataLoaderError as e:\n",
        "        print(\"Found error during data loading\\n{0}\".format(str(e)))\n",
        "        return False\n",
        "\n",
        "\n",
        "def image_segmentation_generator(images_path, segs_path, batch_size,\n",
        "                                 n_classes, input_height, input_width,\n",
        "                                 output_height, output_width,\n",
        "                                 do_augment=False,\n",
        "                                 augmentation_name=\"aug_all\",\n",
        "                                 custom_augmentation=None,\n",
        "                                 other_inputs_paths=None, preprocessing=None,\n",
        "                                 read_image_type=cv2.IMREAD_COLOR , ignore_segs=False ):\n",
        "    \n",
        "\n",
        "    if not ignore_segs:\n",
        "        img_seg_pairs = get_pairs_from_paths(images_path, segs_path, other_inputs_paths=other_inputs_paths)\n",
        "        random.shuffle(img_seg_pairs)\n",
        "        zipped = itertools.cycle(img_seg_pairs)\n",
        "    else:\n",
        "        img_list = get_image_list_from_path( images_path )\n",
        "        random.shuffle( img_list )\n",
        "        img_list_gen = itertools.cycle( img_list )\n",
        "\n",
        "\n",
        "    while True:\n",
        "        X = []\n",
        "        Y = []\n",
        "        for _ in range(batch_size):\n",
        "            if other_inputs_paths is None:\n",
        "\n",
        "                if ignore_segs:\n",
        "                    im = next( img_list_gen )\n",
        "                    seg = None \n",
        "                else:\n",
        "                    im, seg = next(zipped)\n",
        "                    seg = cv2.imread(seg, 1)\n",
        "\n",
        "                im = cv2.imread(im, read_image_type)\n",
        "                \n",
        "\n",
        "                if do_augment:\n",
        "\n",
        "                    assert ignore_segs == False , \"Not supported yet\"\n",
        "\n",
        "                    if custom_augmentation is None:\n",
        "                        im, seg[:, :, 0] = augment_seg(im, seg[:, :, 0],\n",
        "                                                       augmentation_name)\n",
        "                    else:\n",
        "                        im, seg[:, :, 0] = custom_augment_seg(im, seg[:, :, 0],\n",
        "                                                              custom_augmentation)\n",
        "\n",
        "                if preprocessing is not None:\n",
        "                    im = preprocessing(im)\n",
        "\n",
        "                X.append(get_image_array(im, input_width,\n",
        "                                         input_height, ordering=IMAGE_ORDERING))\n",
        "            else:\n",
        "\n",
        "                assert ignore_segs == False , \"Not supported yet\"\n",
        "\n",
        "                im, seg, others = next(zipped)\n",
        "\n",
        "                im = cv2.imread(im, read_image_type)\n",
        "                seg = cv2.imread(seg, 1)\n",
        "\n",
        "                oth = []\n",
        "                for f in others:\n",
        "                    oth.append(cv2.imread(f, read_image_type))\n",
        "\n",
        "                if do_augment:\n",
        "                    if custom_augmentation is None:\n",
        "                        ims, seg[:, :, 0] = augment_seg(im, seg[:, :, 0],\n",
        "                                                        augmentation_name, other_imgs=oth)\n",
        "                    else:\n",
        "                        ims, seg[:, :, 0] = custom_augment_seg(im, seg[:, :, 0],\n",
        "                                                               custom_augmentation, other_imgs=oth)\n",
        "                else:\n",
        "                    ims = [im]\n",
        "                    ims.extend(oth)\n",
        "\n",
        "                oth = []\n",
        "                for i, image in enumerate(ims):\n",
        "                    oth_im = get_image_array(image, input_width,\n",
        "                                             input_height, ordering=IMAGE_ORDERING)\n",
        "\n",
        "                    if preprocessing is not None:\n",
        "                        if isinstance(preprocessing, Sequence):\n",
        "                            oth_im = preprocessing[i](oth_im)\n",
        "                        else:\n",
        "                            oth_im = preprocessing(oth_im)\n",
        "\n",
        "                    oth.append(oth_im)\n",
        "\n",
        "                X.append(oth)\n",
        "\n",
        "            if not ignore_segs:\n",
        "                Y.append(get_segmentation_array(\n",
        "                    seg, n_classes, output_width, output_height))\n",
        "\n",
        "        if ignore_segs:\n",
        "            yield np.array(X)\n",
        "        else:\n",
        "            yield np.array(X), np.array(Y)"
      ],
      "metadata": {
        "id": "ibvgosAhXEJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import six\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "random.seed(DATA_LOADER_SEED)\n",
        "\n",
        "\n",
        "def model_from_checkpoint_path(checkpoints_path):\n",
        "\n",
        "    #from .models.all_models import model_from_name\n",
        "    assert (os.path.isfile(checkpoints_path+\"_config.json\")\n",
        "            ), \"Checkpoint not found.\"\n",
        "    model_config = json.loads(\n",
        "        open(checkpoints_path+\"_config.json\", \"r\").read())\n",
        "    latest_weights = find_latest_checkpoint(checkpoints_path)\n",
        "    assert (latest_weights is not None), \"Checkpoint not found.\"\n",
        "    model = model_from_name[model_config['model_class']](\n",
        "        model_config['n_classes'], input_height=model_config['input_height'],\n",
        "        input_width=model_config['input_width'])\n",
        "    print(\"loaded weights \", latest_weights)\n",
        "    status = model.load_weights(latest_weights)\n",
        "\n",
        "    if status is not None:\n",
        "        status.expect_partial()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_colored_segmentation_image(seg_arr, n_classes, colors=class_colors):\n",
        "    output_height = seg_arr.shape[0]\n",
        "    output_width = seg_arr.shape[1]\n",
        "\n",
        "    seg_img = np.zeros((output_height, output_width, 3))\n",
        "\n",
        "    for c in range(n_classes):\n",
        "        seg_arr_c = seg_arr[:, :] == c\n",
        "        seg_img[:, :, 0] += ((seg_arr_c)*(colors[c][0])).astype('uint8')\n",
        "        seg_img[:, :, 1] += ((seg_arr_c)*(colors[c][1])).astype('uint8')\n",
        "        seg_img[:, :, 2] += ((seg_arr_c)*(colors[c][2])).astype('uint8')\n",
        "\n",
        "    return seg_img\n",
        "\n",
        "\n",
        "def get_legends(class_names, colors=class_colors):\n",
        "\n",
        "    n_classes = len(class_names)\n",
        "    legend = np.zeros(((len(class_names) * 25) + 25, 125, 3),\n",
        "                      dtype=\"uint8\") + 255\n",
        "\n",
        "    class_names_colors = enumerate(zip(class_names[:n_classes],\n",
        "                                       colors[:n_classes]))\n",
        "\n",
        "    for (i, (class_name, color)) in class_names_colors:\n",
        "        color = [int(c) for c in color]\n",
        "        cv2.putText(legend, class_name, (5, (i * 25) + 17),\n",
        "                    cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1)\n",
        "        cv2.rectangle(legend, (100, (i * 25)), (125, (i * 25) + 25),\n",
        "                      tuple(color), -1)\n",
        "\n",
        "    return legend\n",
        "\n",
        "\n",
        "def overlay_seg_image(inp_img, seg_img):\n",
        "    orininal_h = inp_img.shape[0]\n",
        "    orininal_w = inp_img.shape[1]\n",
        "    seg_img = cv2.resize(seg_img, (orininal_w, orininal_h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    fused_img = (inp_img/2 + seg_img/2).astype('uint8')\n",
        "    return fused_img\n",
        "\n",
        "\n",
        "def concat_lenends(seg_img, legend_img):\n",
        "\n",
        "    new_h = np.maximum(seg_img.shape[0], legend_img.shape[0])\n",
        "    new_w = seg_img.shape[1] + legend_img.shape[1]\n",
        "\n",
        "    out_img = np.zeros((new_h, new_w, 3)).astype('uint8') + legend_img[0, 0, 0]\n",
        "\n",
        "    out_img[:legend_img.shape[0], :  legend_img.shape[1]] = np.copy(legend_img)\n",
        "    out_img[:seg_img.shape[0], legend_img.shape[1]:] = np.copy(seg_img)\n",
        "\n",
        "    return out_img\n",
        "\n",
        "\n",
        "def visualize_segmentation(seg_arr, inp_img=None, n_classes=None,\n",
        "                           colors=class_colors, class_names=None,\n",
        "                           overlay_img=False, show_legends=False,\n",
        "                           prediction_width=None, prediction_height=None):\n",
        "\n",
        "    if n_classes is None:\n",
        "        n_classes = np.max(seg_arr)\n",
        "\n",
        "    seg_img = get_colored_segmentation_image(seg_arr, n_classes, colors=colors)\n",
        "\n",
        "    if inp_img is not None:\n",
        "        original_h = inp_img.shape[0]\n",
        "        original_w = inp_img.shape[1]\n",
        "        seg_img = cv2.resize(seg_img, (original_w, original_h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    if (prediction_height is not None) and (prediction_width is not None):\n",
        "        seg_img = cv2.resize(seg_img, (prediction_width, prediction_height), interpolation=cv2.INTER_NEAREST)\n",
        "        if inp_img is not None:\n",
        "            inp_img = cv2.resize(inp_img,\n",
        "                                 (prediction_width, prediction_height))\n",
        "\n",
        "    if overlay_img:\n",
        "        assert inp_img is not None\n",
        "        seg_img = overlay_seg_image(inp_img, seg_img)\n",
        "\n",
        "    if show_legends:\n",
        "        assert class_names is not None\n",
        "        legend_img = get_legends(class_names, colors=colors)\n",
        "\n",
        "        seg_img = concat_lenends(seg_img, legend_img)\n",
        "\n",
        "    return seg_img\n",
        "\n",
        "\n",
        "def predict(model=None, inp=None, out_fname=None,\n",
        "            checkpoints_path=None, overlay_img=False,\n",
        "            class_names=None, show_legends=False, colors=class_colors,\n",
        "            prediction_width=None, prediction_height=None,\n",
        "            read_image_type=1):\n",
        "\n",
        "    if model is None and (checkpoints_path is not None):\n",
        "        model = model_from_checkpoint_path(checkpoints_path)\n",
        "\n",
        "    assert (inp is not None)\n",
        "    assert ((type(inp) is np.ndarray) or isinstance(inp, six.string_types)),\\\n",
        "        \"Input should be the CV image or the input file name\"\n",
        "\n",
        "    if isinstance(inp, six.string_types):\n",
        "        inp = cv2.imread(inp, read_image_type)\n",
        "\n",
        "    assert (len(inp.shape) == 3 or len(inp.shape) == 1 or len(inp.shape) == 4), \"Image should be h,w,3 \"\n",
        "\n",
        "    output_width = model.output_width\n",
        "    output_height = model.output_height\n",
        "    input_width = model.input_width\n",
        "    input_height = model.input_height\n",
        "    n_classes = model.n_classes\n",
        "\n",
        "    x = get_image_array(inp, input_width, input_height,\n",
        "                        ordering=IMAGE_ORDERING)\n",
        "    pr = model.predict(np.array([x]))[0]\n",
        "    pr = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)\n",
        "\n",
        "    seg_img = visualize_segmentation(pr, inp, n_classes=n_classes,\n",
        "                                     colors=colors, overlay_img=overlay_img,\n",
        "                                     show_legends=show_legends,\n",
        "                                     class_names=class_names,\n",
        "                                     prediction_width=prediction_width,\n",
        "                                     prediction_height=prediction_height)\n",
        "\n",
        "    if out_fname is not None:\n",
        "        cv2.imwrite(out_fname, seg_img)\n",
        "\n",
        "    return pr\n",
        "\n",
        "\n",
        "def predict_multiple(model=None, inps=None, inp_dir=None, out_dir=None,\n",
        "                     checkpoints_path=None, overlay_img=False,\n",
        "                     class_names=None, show_legends=False, colors=class_colors,\n",
        "                     prediction_width=None, prediction_height=None, read_image_type=1):\n",
        "\n",
        "    if model is None and (checkpoints_path is not None):\n",
        "        model = model_from_checkpoint_path(checkpoints_path)\n",
        "\n",
        "    if inps is None and (inp_dir is not None):\n",
        "        inps = glob.glob(os.path.join(inp_dir, \"*.jpg\")) + glob.glob(\n",
        "            os.path.join(inp_dir, \"*.png\")) + \\\n",
        "            glob.glob(os.path.join(inp_dir, \"*.jpeg\"))\n",
        "        inps = sorted(inps)\n",
        "\n",
        "    assert type(inps) is list\n",
        "\n",
        "    all_prs = []\n",
        "\n",
        "    if not out_dir is None:\n",
        "        if not os.path.exists(out_dir):\n",
        "            os.makedirs(out_dir)\n",
        "\n",
        "\n",
        "    for i, inp in enumerate(tqdm(inps)):\n",
        "        if out_dir is None:\n",
        "            out_fname = None\n",
        "        else:\n",
        "            if isinstance(inp, six.string_types):\n",
        "                out_fname = os.path.join(out_dir, os.path.basename(inp))\n",
        "            else:\n",
        "                out_fname = os.path.join(out_dir, str(i) + \".jpg\")\n",
        "\n",
        "        pr = predict(model, inp, out_fname,\n",
        "                     overlay_img=overlay_img, class_names=class_names,\n",
        "                     show_legends=show_legends, colors=colors,\n",
        "                     prediction_width=prediction_width,\n",
        "                     prediction_height=prediction_height, read_image_type=read_image_type)\n",
        "\n",
        "        all_prs.append(pr)\n",
        "\n",
        "    return all_prs\n",
        "\n",
        "\n",
        "def set_video(inp, video_name):\n",
        "    cap = cv2.VideoCapture(inp)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    size = (video_width, video_height)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
        "    video = cv2.VideoWriter(video_name, fourcc, fps, size)\n",
        "    return cap, video, fps\n",
        "\n",
        "\n",
        "def predict_video(model=None, inp=None, output=None,\n",
        "                  checkpoints_path=None, display=False, overlay_img=True,\n",
        "                  class_names=None, show_legends=False, colors=class_colors,\n",
        "                  prediction_width=None, prediction_height=None):\n",
        "\n",
        "    if model is None and (checkpoints_path is not None):\n",
        "        model = model_from_checkpoint_path(checkpoints_path)\n",
        "    n_classes = model.n_classes\n",
        "\n",
        "    cap, video, fps = set_video(inp, output)\n",
        "    while(cap.isOpened()):\n",
        "        prev_time = time()\n",
        "        ret, frame = cap.read()\n",
        "        if frame is not None:\n",
        "            pr = predict(model=model, inp=frame)\n",
        "            fused_img = visualize_segmentation(\n",
        "                pr, frame, n_classes=n_classes,\n",
        "                colors=colors,\n",
        "                overlay_img=overlay_img,\n",
        "                show_legends=show_legends,\n",
        "                class_names=class_names,\n",
        "                prediction_width=prediction_width,\n",
        "                prediction_height=prediction_height\n",
        "                )\n",
        "        else:\n",
        "            break\n",
        "        print(\"FPS: {}\".format(1/(time() - prev_time)))\n",
        "        if output is not None:\n",
        "            video.write(fused_img)\n",
        "        if display:\n",
        "            cv2.imshow('Frame masked', fused_img)\n",
        "            if cv2.waitKey(fps) & 0xFF == ord('q'):\n",
        "                break\n",
        "    cap.release()\n",
        "    if output is not None:\n",
        "        video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "def evaluate(model=None, inp_images=None, annotations=None,\n",
        "             inp_images_dir=None, annotations_dir=None, checkpoints_path=None, read_image_type=1):\n",
        "\n",
        "    if model is None:\n",
        "        assert (checkpoints_path is not None),\\\n",
        "                \"Please provide the model or the checkpoints_path\"\n",
        "        model = model_from_checkpoint_path(checkpoints_path)\n",
        "\n",
        "    if inp_images is None:\n",
        "        assert (inp_images_dir is not None),\\\n",
        "                \"Please provide inp_images or inp_images_dir\"\n",
        "        assert (annotations_dir is not None),\\\n",
        "            \"Please provide inp_images or inp_images_dir\"\n",
        "\n",
        "        paths = get_pairs_from_paths(inp_images_dir, annotations_dir)\n",
        "        paths = list(zip(*paths))\n",
        "        inp_images = list(paths[0])\n",
        "        annotations = list(paths[1])\n",
        "\n",
        "    assert type(inp_images) is list\n",
        "    assert type(annotations) is list\n",
        "\n",
        "    tp = np.zeros(model.n_classes)\n",
        "    fp = np.zeros(model.n_classes)\n",
        "    fn = np.zeros(model.n_classes)\n",
        "    n_pixels = np.zeros(model.n_classes)\n",
        "\n",
        "    for inp, ann in tqdm(zip(inp_images, annotations)):\n",
        "        pr = predict(model, inp, read_image_type=read_image_type)\n",
        "        gt = get_segmentation_array(ann, model.n_classes,\n",
        "                                    model.output_width, model.output_height,\n",
        "                                    no_reshape=True, read_image_type=read_image_type)\n",
        "        gt = gt.argmax(-1)\n",
        "        pr = pr.flatten()\n",
        "        gt = gt.flatten()\n",
        "\n",
        "        for cl_i in range(model.n_classes):\n",
        "\n",
        "            tp[cl_i] += np.sum((pr == cl_i) * (gt == cl_i))\n",
        "            fp[cl_i] += np.sum((pr == cl_i) * ((gt != cl_i)))\n",
        "            fn[cl_i] += np.sum((pr != cl_i) * ((gt == cl_i)))\n",
        "            n_pixels[cl_i] += np.sum(gt == cl_i)\n",
        "\n",
        "    cl_wise_score = tp / (tp + fp + fn + 0.000000000001)\n",
        "    n_pixels_norm = n_pixels / np.sum(n_pixels)\n",
        "    frequency_weighted_IU = np.sum(cl_wise_score*n_pixels_norm)\n",
        "    mean_IU = np.mean(cl_wise_score)\n",
        "\n",
        "    return {\n",
        "        \"frequency_weighted_IU\": frequency_weighted_IU,\n",
        "        \"mean_IU\": mean_IU,\n",
        "        \"class_wise_IU\": cl_wise_score\n",
        "    }"
      ],
      "metadata": {
        "id": "o8_HreBUW5Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import six\n",
        "from keras.callbacks import Callback\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import sys\n",
        "\n",
        "def find_latest_checkpoint(checkpoints_path, fail_safe=True):\n",
        "\n",
        "    # This is legacy code, there should always be a \"checkpoint\" file in your directory\n",
        "\n",
        "    def get_epoch_number_from_path(path):\n",
        "        return path.replace(checkpoints_path, \"\").strip(\".\")\n",
        "\n",
        "    # Get all matching files\n",
        "    all_checkpoint_files = glob.glob(checkpoints_path + \".*\")\n",
        "    if len(all_checkpoint_files) == 0:\n",
        "        all_checkpoint_files = glob.glob(checkpoints_path + \"*.*\")\n",
        "    all_checkpoint_files = [ff.replace(\".index\", \"\") for ff in\n",
        "                            all_checkpoint_files]  # to make it work for newer versions of keras\n",
        "    # Filter out entries where the epoc_number part is pure number\n",
        "    all_checkpoint_files = list(filter(lambda f: get_epoch_number_from_path(f)\n",
        "                                       .isdigit(), all_checkpoint_files))\n",
        "    if not len(all_checkpoint_files):\n",
        "        # The glob list is empty, don't have a checkpoints_path\n",
        "        if not fail_safe:\n",
        "            raise ValueError(\"Checkpoint path {0} invalid\"\n",
        "                             .format(checkpoints_path))\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Find the checkpoint file with the maximum epoch\n",
        "    latest_epoch_checkpoint = max(all_checkpoint_files,\n",
        "                                  key=lambda f:\n",
        "                                  int(get_epoch_number_from_path(f)))\n",
        "\n",
        "    return latest_epoch_checkpoint\n",
        "\n",
        "def masked_categorical_crossentropy(gt, pr):\n",
        "    from keras.losses import categorical_crossentropy\n",
        "    mask = 1 - gt[:, :, 0]\n",
        "    return categorical_crossentropy(gt, pr) * mask\n",
        "\n",
        "\n",
        "class CheckpointsCallback(Callback):\n",
        "    def __init__(self, checkpoints_path):\n",
        "        self.checkpoints_path = checkpoints_path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.checkpoints_path is not None:\n",
        "            self.model.save_weights(self.checkpoints_path + \".\" + str(epoch))\n",
        "            print(\"saved \", self.checkpoints_path + \".\" + str(epoch))\n",
        "\n",
        "\n",
        "def train(model,\n",
        "          train_images,\n",
        "          train_annotations,\n",
        "          input_height=None,\n",
        "          input_width=None,\n",
        "          n_classes=None,\n",
        "          verify_dataset=True,\n",
        "          checkpoints_path=None,\n",
        "          epochs=5,\n",
        "          batch_size=2,\n",
        "          validate=False,\n",
        "          val_images=None,\n",
        "          val_annotations=None,\n",
        "          val_batch_size=2,\n",
        "          auto_resume_checkpoint=False,\n",
        "          load_weights=None,\n",
        "          steps_per_epoch=512,\n",
        "          val_steps_per_epoch=512,\n",
        "          gen_use_multiprocessing=False,\n",
        "          ignore_zero_class=False,\n",
        "          optimizer_name='adam',\n",
        "          do_augment=False,\n",
        "          augmentation_name=\"aug_all\",\n",
        "          callbacks=None,\n",
        "          custom_augmentation=None,\n",
        "          other_inputs_paths=None,\n",
        "          preprocessing=None,\n",
        "          read_image_type=1  # cv2.IMREAD_COLOR = 1 (rgb),\n",
        "                             # cv2.IMREAD_GRAYSCALE = 0,\n",
        "                             # cv2.IMREAD_UNCHANGED = -1 (4 channels like RGBA)\n",
        "         ):\n",
        "    from .models.all_models import model_from_name\n",
        "    # check if user gives model name instead of the model object\n",
        "    if isinstance(model, six.string_types):\n",
        "        # create the model from the name\n",
        "        assert (n_classes is not None), \"Please provide the n_classes\"\n",
        "        if (input_height is not None) and (input_width is not None):\n",
        "            model = model_from_name[model](\n",
        "                n_classes, input_height=input_height, input_width=input_width)\n",
        "        else:\n",
        "            model = model_from_name[model](n_classes)\n",
        "\n",
        "    n_classes = model.n_classes\n",
        "    input_height = model.input_height\n",
        "    input_width = model.input_width\n",
        "    output_height = model.output_height\n",
        "    output_width = model.output_width\n",
        "\n",
        "    if validate:\n",
        "        assert val_images is not None\n",
        "        assert val_annotations is not None\n",
        "\n",
        "    if optimizer_name is not None:\n",
        "\n",
        "        if ignore_zero_class:\n",
        "            loss_k = masked_categorical_crossentropy\n",
        "        else:\n",
        "            loss_k = 'categorical_crossentropy'\n",
        "\n",
        "        model.compile(loss=loss_k,\n",
        "                      optimizer=optimizer_name,\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    if checkpoints_path is not None:\n",
        "        config_file = checkpoints_path + \"_config.json\"\n",
        "        dir_name = os.path.dirname(config_file)\n",
        "\n",
        "        if ( not os.path.exists(dir_name) )  and len( dir_name ) > 0 :\n",
        "            os.makedirs(dir_name)\n",
        "\n",
        "        with open(config_file, \"w\") as f:\n",
        "            json.dump({\n",
        "                \"model_class\": model.model_name,\n",
        "                \"n_classes\": n_classes,\n",
        "                \"input_height\": input_height,\n",
        "                \"input_width\": input_width,\n",
        "                \"output_height\": output_height,\n",
        "                \"output_width\": output_width\n",
        "            }, f)\n",
        "\n",
        "    if load_weights is not None and len(load_weights) > 0:\n",
        "        print(\"Loading weights from \", load_weights)\n",
        "        model.load_weights(load_weights)\n",
        "\n",
        "    initial_epoch = 0\n",
        "\n",
        "    if auto_resume_checkpoint and (checkpoints_path is not None):\n",
        "        latest_checkpoint = find_latest_checkpoint(checkpoints_path)\n",
        "        if latest_checkpoint is not None:\n",
        "            print(\"Loading the weights from latest checkpoint \",\n",
        "                  latest_checkpoint)\n",
        "            model.load_weights(latest_checkpoint)\n",
        "\n",
        "            initial_epoch = int(latest_checkpoint.split('.')[-1])\n",
        "\n",
        "    if verify_dataset:\n",
        "        print(\"Verifying training dataset\")\n",
        "        verified = verify_segmentation_dataset(train_images,\n",
        "                                               train_annotations,\n",
        "                                               n_classes)\n",
        "        assert verified\n",
        "        if validate:\n",
        "            print(\"Verifying validation dataset\")\n",
        "            verified = verify_segmentation_dataset(val_images,\n",
        "                                                   val_annotations,\n",
        "                                                   n_classes)\n",
        "            assert verified\n",
        "\n",
        "    train_gen = image_segmentation_generator(\n",
        "        train_images, train_annotations,  batch_size,  n_classes,\n",
        "        input_height, input_width, output_height, output_width,\n",
        "        do_augment=do_augment, augmentation_name=augmentation_name,\n",
        "        custom_augmentation=custom_augmentation, other_inputs_paths=other_inputs_paths,\n",
        "        preprocessing=preprocessing, read_image_type=read_image_type)\n",
        "\n",
        "    if validate:\n",
        "        val_gen = image_segmentation_generator(\n",
        "            val_images, val_annotations,  val_batch_size,\n",
        "            n_classes, input_height, input_width, output_height, output_width,\n",
        "            other_inputs_paths=other_inputs_paths,\n",
        "            preprocessing=preprocessing, read_image_type=read_image_type)\n",
        "\n",
        "    if callbacks is None and (not checkpoints_path is  None) :\n",
        "        default_callback = ModelCheckpoint(\n",
        "                filepath=checkpoints_path + \".{epoch:05d}\",\n",
        "                save_weights_only=True,\n",
        "                verbose=True\n",
        "            )\n",
        "\n",
        "        if sys.version_info[0] < 3: # for pyhton 2 \n",
        "            default_callback = CheckpointsCallback(checkpoints_path)\n",
        "\n",
        "        callbacks = [\n",
        "            default_callback\n",
        "        ]\n",
        "\n",
        "    if callbacks is None:\n",
        "        callbacks = []\n",
        "\n",
        "    if not validate:\n",
        "        model.fit(train_gen, steps_per_epoch=steps_per_epoch,\n",
        "                  epochs=epochs, callbacks=callbacks, initial_epoch=initial_epoch)\n",
        "    else:\n",
        "        model.fit(train_gen,\n",
        "                  steps_per_epoch=steps_per_epoch,\n",
        "                  validation_data=val_gen,\n",
        "                  validation_steps=val_steps_per_epoch,\n",
        "                  epochs=epochs, callbacks=callbacks,\n",
        "                  use_multiprocessing=gen_use_multiprocessing, initial_epoch=initial_epoch)"
      ],
      "metadata": {
        "id": "kGdHWNsAWtdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from types import MethodType\n",
        "\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "import keras.backend as K\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# source m1 , dest m2\n",
        "def transfer_weights(m1, m2, verbose=True):\n",
        "\n",
        "    assert len(m1.layers) == len(\n",
        "        m2.layers), \"Both models should have same number of layers\"\n",
        "\n",
        "    nSet = 0\n",
        "    nNotSet = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Copying weights \")\n",
        "        bar = tqdm(zip(m1.layers, m2.layers))\n",
        "    else:\n",
        "        bar = zip(m1.layers, m2.layers)\n",
        "\n",
        "    for l, ll in bar:\n",
        "\n",
        "        if not any([w.shape != ww.shape for w, ww in zip(list(l.weights),\n",
        "                                                         list(ll.weights))]):\n",
        "            if len(list(l.weights)) > 0:\n",
        "                ll.set_weights(l.get_weights())\n",
        "                nSet += 1\n",
        "        else:\n",
        "            nNotSet += 1\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Copied weights of %d layers and skipped %d layers\" %\n",
        "              (nSet, nNotSet))\n",
        "\n",
        "\n",
        "def resize_image(inp,  s, data_format):\n",
        "\n",
        "    try:\n",
        "\n",
        "        return Lambda(lambda x: K.resize_images(x,\n",
        "                                                height_factor=s[0],\n",
        "                                                width_factor=s[1],\n",
        "                                                data_format=data_format,\n",
        "                                                interpolation='bilinear'))(inp)\n",
        "\n",
        "    except Exception as e:\n",
        "        # if keras is old, then rely on the tf function\n",
        "        # Sorry theano/cntk users!!!\n",
        "        assert data_format == 'channels_last'\n",
        "        assert IMAGE_ORDERING == 'channels_last'\n",
        "\n",
        "        import tensorflow as tf\n",
        "\n",
        "        return Lambda(\n",
        "            lambda x: tf.image.resize_images(\n",
        "                x, (K.int_shape(x)[1]*s[0], K.int_shape(x)[2]*s[1]))\n",
        "        )(inp)\n",
        "\n",
        "\n",
        "def get_segmentation_model(input, output):\n",
        "\n",
        "    img_input = input\n",
        "    o = output\n",
        "\n",
        "    o_shape = Model(img_input, o).output_shape\n",
        "    i_shape = Model(img_input, o).input_shape\n",
        "\n",
        "    if IMAGE_ORDERING == 'channels_first':\n",
        "        output_height = o_shape[2]\n",
        "        output_width = o_shape[3]\n",
        "        input_height = i_shape[2]\n",
        "        input_width = i_shape[3]\n",
        "        n_classes = o_shape[1]\n",
        "        o = (Reshape((-1, output_height*output_width)))(o)\n",
        "        o = (Permute((2, 1)))(o)\n",
        "    elif IMAGE_ORDERING == 'channels_last':\n",
        "        output_height = o_shape[1]\n",
        "        output_width = o_shape[2]\n",
        "        input_height = i_shape[1]\n",
        "        input_width = i_shape[2]\n",
        "        n_classes = o_shape[3]\n",
        "        o = (Reshape((output_height*output_width, -1)))(o)\n",
        "\n",
        "    o = (Activation('softmax'))(o)\n",
        "    model = Model(img_input, o)\n",
        "    model.output_width = output_width\n",
        "    model.output_height = output_height\n",
        "    model.n_classes = n_classes\n",
        "    model.input_height = input_height\n",
        "    model.input_width = input_width\n",
        "    model.model_name = \"\"\n",
        "\n",
        "    model.train = MethodType(train, model)\n",
        "    model.predict_segmentation = MethodType(predict, model)\n",
        "    model.predict_multiple = MethodType(predict_multiple, model)\n",
        "    model.evaluate_segmentation = MethodType(evaluate, model)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZkxK_aWgUdNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def segnet_decoder(f, n_classes, n_up=3):\n",
        "\n",
        "    assert n_up >= 2\n",
        "\n",
        "    o = f\n",
        "    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
        "    o = (Conv2D(512, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n",
        "    o = (BatchNormalization())(o)\n",
        "\n",
        "    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n",
        "    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
        "    o = (Conv2D(256, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n",
        "    o = (BatchNormalization())(o)\n",
        "\n",
        "    for _ in range(n_up-2):\n",
        "        o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n",
        "        o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
        "        o = (Conv2D(128, (3, 3), padding='valid',\n",
        "             data_format=IMAGE_ORDERING))(o)\n",
        "        o = (BatchNormalization())(o)\n",
        "\n",
        "    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n",
        "    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n",
        "    o = (Conv2D(64, (3, 3), padding='valid', data_format=IMAGE_ORDERING, name=\"seg_feats\"))(o)\n",
        "    o = (BatchNormalization())(o)\n",
        "\n",
        "    o = Conv2D(n_classes, (3, 3), padding='same',\n",
        "               data_format=IMAGE_ORDERING)(o)\n",
        "\n",
        "    return o\n",
        "\n",
        "\n",
        "def _segnet(n_classes, encoder,  input_height=416, input_width=608,\n",
        "            encoder_level=3, channels=3):\n",
        "\n",
        "    img_input, levels = encoder(\n",
        "        input_height=input_height,  input_width=input_width, channels=channels)\n",
        "\n",
        "    feat = levels[encoder_level]\n",
        "    o = segnet_decoder(feat, n_classes, n_up=3)\n",
        "    model = get_segmentation_model(img_input, o)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def segnet(n_classes, input_height=416, input_width=608, encoder_level=3, channels=3):\n",
        "\n",
        "    model = _segnet(n_classes, vanilla_encoder,  input_height=input_height,\n",
        "                    input_width=input_width, encoder_level=encoder_level, channels=channels)\n",
        "    model.model_name = \"segnet\"\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg_segnet(n_classes, input_height=416, input_width=608, encoder_level=3, channels=3):\n",
        "\n",
        "    model = _segnet(n_classes, get_vgg_encoder,  input_height=input_height,\n",
        "                    input_width=input_width, encoder_level=encoder_level, channels=channels)\n",
        "    model.model_name = \"vgg_segnet\"\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50_segnet(n_classes, input_height=416, input_width=608,\n",
        "                    encoder_level=3, channels=3):\n",
        "\n",
        "    model = _segnet(n_classes, get_resnet50_encoder, input_height=input_height,\n",
        "                    input_width=input_width, encoder_level=encoder_level, channels=channels)\n",
        "    model.model_name = \"resnet50_segnet\"\n",
        "    return model\n",
        "\n",
        "\n",
        "def mobilenet_segnet(n_classes, input_height=224, input_width=224,\n",
        "                     encoder_level=3, channels=3):\n",
        "\n",
        "    model = _segnet(n_classes, get_mobilenet_encoder,\n",
        "                    input_height=input_height,\n",
        "                    input_width=input_width, encoder_level=encoder_level, channels=channels)\n",
        "    model.model_name = \"mobilenet_segnet\"\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    m = vgg_segnet(101)\n",
        "    m = segnet(101)\n",
        "    # m = mobilenet_segnet( 101 )\n",
        "    # from keras.utils import plot_model\n",
        "    # plot_model( m , show_shapes=True , to_file='model.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "WUwCRUimTOa6",
        "outputId": "e889ed3c-9f52-40db-92f8-dd792e12af8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-264cf283079f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_segnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# m = mobilenet_segnet( 101 )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-264cf283079f>\u001b[0m in \u001b[0;36mvgg_segnet\u001b[0;34m(n_classes, input_height, input_width, encoder_level, channels)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     model = _segnet(n_classes, get_vgg_encoder,  input_height=input_height,\n\u001b[0;32m---> 64\u001b[0;31m                     input_width=input_width, encoder_level=encoder_level, channels=channels)\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"vgg_segnet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-264cf283079f>\u001b[0m in \u001b[0;36m_segnet\u001b[0;34m(n_classes, encoder, input_height, input_width, encoder_level, channels)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     img_input, levels = encoder(\n\u001b[0;32m---> 44\u001b[0;31m         input_height=input_height,  input_width=input_width, channels=channels)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_level\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-33cda47eae81>\u001b[0m in \u001b[0;36mget_vgg_encoder\u001b[0;34m(input_height, input_width, pretrained, channels)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         VGG_Weights_path = keras.utils.get_file(\n\u001b[0m\u001b[1;32m     74\u001b[0m             pretrained_url.split(\"/\")[-1], pretrained_url)\n\u001b[1;32m     75\u001b[0m         \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVGG_Weights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.utils' has no attribute 'get_file'"
          ]
        }
      ]
    }
  ]
}